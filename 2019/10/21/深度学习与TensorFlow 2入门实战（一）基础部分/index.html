<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">

<script>
    (function () {
        if ('') {
            if (prompt('请输入文章密码') !== '') {
                alert('密码错误！');
                if (history.length === 1) {
                    location.replace("http://xiaohuihui1024.github.io/"); // 这里替换成你的首页
                } else {
                    history.back();
                }
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon32.ico?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon16.ico?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.png?v=7.4.0" color="#222">
  <link rel="alternate" href="/rss.xml" title="xiaohuihui 个人博客" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: true,
    sidebar: {"position":"left","display":"post","offset":18,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="看TensorFlow 2.0入门视频过程中记录的笔记和部分代码，此篇为TensorFlow2.x 的一些介绍和操作。由于前期经验不足，一些废话可能都记录下来了。后续笔记中尽量避免此问题。另外，关于API的部分，官方文档的说明可能会更好~">
<meta name="keywords" content="深度学习,TensorFlow 2.0">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习与TensorFlow 2入门实战（一）基础部分">
<meta property="og:url" content="https://xiaohuihui1024.github.io/2019/10/21/深度学习与TensorFlow 2入门实战（一）基础部分/index.html">
<meta property="og:site_name" content="xiaohuihui 个人博客">
<meta property="og:description" content="看TensorFlow 2.0入门视频过程中记录的笔记和部分代码，此篇为TensorFlow2.x 的一些介绍和操作。由于前期经验不足，一些废话可能都记录下来了。后续笔记中尽量避免此问题。另外，关于API的部分，官方文档的说明可能会更好~">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://xiaohuihui1024.github.io/images/truncated1.jpg">
<meta property="og:image" content="https://xiaohuihui1024.github.io/images/NLP1.jpg">
<meta property="og:image" content="https://xiaohuihui1024.github.io/images/Broadcasting-1.jpg">
<meta property="og:updated_time" content="2019-11-10T13:46:14.460Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习与TensorFlow 2入门实战（一）基础部分">
<meta name="twitter:description" content="看TensorFlow 2.0入门视频过程中记录的笔记和部分代码，此篇为TensorFlow2.x 的一些介绍和操作。由于前期经验不足，一些废话可能都记录下来了。后续笔记中尽量避免此问题。另外，关于API的部分，官方文档的说明可能会更好~">
<meta name="twitter:image" content="https://xiaohuihui1024.github.io/images/truncated1.jpg">
  <link rel="canonical" href="https://xiaohuihui1024.github.io/2019/10/21/深度学习与TensorFlow 2入门实战（一）基础部分/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>深度学习与TensorFlow 2入门实战（一）基础部分 | xiaohuihui 个人博客</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaohuihui 个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">低调做人，高调做事</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-sitemap">
      
    

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="fa fa-search fa-fw"></i>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL3hpYW9odWlodWkxMDI0" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://xiaohuihui1024.github.io/2019/10/21/深度学习与TensorFlow 2入门实战（一）基础部分/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiaohuihui">
      <meta itemprop="description" content="记录学习历程，分享技术心得。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaohuihui 个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">深度学习与TensorFlow 2入门实战（一）基础部分

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-10-21 16:27:44" itemprop="dateCreated datePublished" datetime="2019-10-21T16:27:44+08:00">2019-10-21</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-11-10 21:46:14" itemprop="dateModified" datetime="2019-11-10T21:46:14+08:00">2019-11-10</time>
              </span>
            
          

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          
          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span>50k</span>
            </span>
          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span>1:32</span>
            </span>
          
            <div class="post-description">看TensorFlow 2.0入门视频过程中记录的笔记和部分代码，此篇为TensorFlow2.x 的一些介绍和操作。由于前期经验不足，一些废话可能都记录下来了。后续笔记中尽量避免此问题。另外，关于API的部分，官方文档的说明可能会更好~</div>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="深度学习初见"><a href="#深度学习初见" class="headerlink" title="深度学习初见"></a>深度学习初见</h2><p>以深度学习的算法讲解为主，以 TensorFlow 实践为辅，通过理论和实践的方法确保能理解算法。</p>
<h3 id="TensorFlow-简介"><a href="#TensorFlow-简介" class="headerlink" title="TensorFlow 简介"></a>TensorFlow 简介</h3><ul>
<li>2015.9 发布 0.1版本</li>
<li>2017.2 发布 1.0版本</li>
<li>2019春发布2.0版本</li>
</ul>
<h3 id="TensorFlow-vs-PyTorch"><a href="#TensorFlow-vs-PyTorch" class="headerlink" title="TensorFlow vs PyTorch"></a>TensorFlow vs PyTorch</h3><p>计算图 等概念使得用户使用起来困扰。</p>
<h3 id="TensorFlow-2-0"><a href="#TensorFlow-2-0" class="headerlink" title="TensorFlow 2.0"></a>TensorFlow 2.0</h3><p>Easy to use.</p>
<p>去掉图 等概念，这些概念使得 tf1 使用起来非常麻烦</p>
<ul>
<li>session.run | 去掉session概念</li>
<li>tf.control_dependencies | 去掉实施控制</li>
<li>tf.global_variables_initializer | 之前。变量命名体系 分 Python的和 其内部的</li>
<li>tf.cond, tf.while_loop | 去掉分支控制，这些可以用 python的 if..else完成，但是1.x 硬生生造了这样的概念</li>
</ul>
<p>变的 像 pytorch一样 所见即所得，比如</p>
<blockquote>
<p>tensorflow 在 各种排行都 比 pytorch 高</p>
</blockquote>
<h3 id="忘掉1-0"><a href="#忘掉1-0" class="headerlink" title="忘掉1.0"></a>忘掉1.0</h3><ul>
<li>计算图 Graph</li>
<li>会话 Session</li>
<li>变量管理 Variable 与 共享 reuse</li>
<li>define-and-run</li>
<li>等等一系列烦人的概念将一去不复返</li>
</ul>
<ul>
<li><p>我们假设你对 TensorFlow 一无所知</p>
<p>1.0 的概念在 2.0中用不到，2.0是自我革新，用户使用起来会丝滑顺畅</p>
</li>
</ul>
<h3 id="TensorFlow-2-0-eco-system"><a href="#TensorFlow-2-0-eco-system" class="headerlink" title="TensorFlow 2.0 eco-system"></a>TensorFlow 2.0 eco-system</h3><ul>
<li>TensorFlow 核心开源库 （本身）<ul>
<li>tf.function 方便的将动态图语言 转变为 静态图，某种程度上进行计算加速</li>
</ul>
</li>
<li>面向网页 JavaScript 的 TensorFlow.js</li>
<li>针对移动设备 和 IoT 的 TensorFlow Lite</li>
<li>面向工业部署的 TensorFlow Extended</li>
</ul>
<p>另外Google 开发了 针对 TensorFlow 的加速硬件 TPU</p>
<ul>
<li>TensorFlow Prob</li>
<li>TPU Cloud</li>
</ul>
<h3 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h3><ul>
<li>忘掉 1.x</li>
<li>PyTorch 和 TensorFlow 选一主修（融会贯通）<ul>
<li>二者都要掌握</li>
</ul>
</li>
<li>Keras 逐渐淡出<ul>
<li>TF + Keras<ul>
<li>tf.keras 本质上是 tf，只不过函数命名参考了 Keras 的命名方式，使用户使用更加方便</li>
</ul>
</li>
<li>PyTorch + Caffe2</li>
</ul>
</li>
</ul>
<blockquote>
<p>把时间放到 深度学习 算法上</p>
</blockquote>
<h3 id="why-TensorFlow"><a href="#why-TensorFlow" class="headerlink" title="why TensorFlow"></a>why TensorFlow</h3><ul>
<li><p>GPU加速</p>
</li>
<li><p>自动求导</p>
<p>不管网络多么深，参数多复杂，总是可以通过自动求导工具帮助我们精确的求出每个权值的梯度</p>
</li>
<li><p>神经网络 API</p>
</li>
</ul>
<h2 id="开发环境安装"><a href="#开发环境安装" class="headerlink" title="开发环境安装"></a>开发环境安装</h2><h3 id="Step1-Anaconda"><a href="#Step1-Anaconda" class="headerlink" title="Step1 Anaconda"></a>Step1 Anaconda</h3><h3 id="Step2-CUDA-10-0"><a href="#Step2-CUDA-10-0" class="headerlink" title="Step2 CUDA 10.0"></a>Step2 CUDA 10.0</h3><ul>
<li>NVIDIA显卡<ul>
<li>注意必须是 NVIDIA 显卡，其他的不能安装</li>
<li>后续安装 TensorFlow 必须用 CPU 版本，前期可以进行小案例的学习，后期稍微中型、大型的案例完全不能用 CPU 了，所以建议买一台 该显卡 的 电脑</li>
<li>选购<ul>
<li>GTX 1060 6GB（入门）</li>
<li>GTX 1080Ti 11GB</li>
<li>GTX 20系列性价比不行</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Step3-TensorFlow安装"><a href="#Step3-TensorFlow安装" class="headerlink" title="Step3. TensorFlow安装"></a>Step3. TensorFlow安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cpu version</span></span><br><span class="line">$ pip install --upgrade tensorflow</span><br><span class="line"><span class="comment"># gpu version</span></span><br><span class="line">$ pip install --upgrade tensorflow-gpu</span><br></pre></td></tr></table></figure>
<p>测试代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># cpu版本测试</span></span><br><span class="line">tf.constant(<span class="number">1.</span>) + tf.constant(<span class="number">2.</span>) </span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=2, shape=(), dtype=float32, numpy=3.0&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gpu版本测试</span></span><br><span class="line">tf.test.is_gpu_available() <span class="comment"># True or False</span></span><br></pre></td></tr></table></figure>
<h2 id="TensorFlow-2-0-基本操作"><a href="#TensorFlow-2-0-基本操作" class="headerlink" title="TensorFlow 2.0 基本操作"></a>TensorFlow 2.0 基本操作</h2><h3 id="数据载体比较"><a href="#数据载体比较" class="headerlink" title="数据载体比较"></a>数据载体比较</h3><ul>
<li><p>list</p>
<p>存储空间大，处理效率低</p>
</li>
<li><p>np.array</p>
<ul>
<li>解决大数据的吞吐，对相同类型的数据，比较方便存储，做运算（转置，加减乘除）</li>
<li>致命弱点：深度学习之前就设计好的科学计算库，没有很好的GPU运算支持，不支持自动求导</li>
</ul>
</li>
<li><p>tf.Tensor</p>
<ul>
<li>地位跟 np.array 有些对等，拼接、分裂、stack操作、random操作类似，为了降低初学者难度，API设计与 NumPy 相近</li>
<li>更加注重神经网络计算</li>
<li>专门的数据载体，存储大量数据</li>
</ul>
</li>
</ul>
<h3 id="What-‘s-Tensor"><a href="#What-‘s-Tensor" class="headerlink" title="What ‘s Tensor"></a>What ‘s Tensor</h3><p>把数据叫为 Tensor</p>
<ul>
<li>scalar: 1.1 标量，准确的数据类型 (dim=0)</li>
<li>vector: [1.1], [1.1, 1.2]</li>
<li>matrix</li>
<li>tensor: $rank$ &gt; 2</li>
</ul>
<h3 id="Tensor-Flow-in-Graph"><a href="#Tensor-Flow-in-Graph" class="headerlink" title="Tensor Flow in Graph"></a>Tensor Flow in Graph</h3><p>名字由来：由于Tensor是数据的载体，非常形象的表明了在神经网络 / DP 中间自由的Flow</p>
<p>Tensor 经过不同的运算 在 网络中间 不停的流动，最终得到结果</p>
<h3 id="TF-is-computing-lib"><a href="#TF-is-computing-lib" class="headerlink" title="TF is computing lib"></a>TF is computing lib</h3><ul>
<li>int, float, double</li>
<li>bool ( eg. equal 返回 bool )</li>
<li>string</li>
</ul>
<h3 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h3><h4 id="From-NumPy，List"><a href="#From-NumPy，List" class="headerlink" title="From NumPy，List"></a>From NumPy，List</h4><p>现有的 NumPy ，list 转换得到</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.convert_to_tensor(np.ones([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=134, shape=(2, 3), dtype=float64, numpy=</span></span><br><span class="line"><span class="string">array([[1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1.]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.convert_to_tensor(np.zeros([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=143, shape=(2, 3), dtype=float64, numpy=</span></span><br><span class="line"><span class="string">array([[0., 0., 0.],</span></span><br><span class="line"><span class="string">       [0., 0., 0.]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>传入 list 示例，list 必须是可以转化为 NumPy 的数据（逻辑上说得通）才能转化为 Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.convert_to_tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=152, shape=(2,), dtype=int32, numpy=array([1, 2])&gt;</span></span><br><span class="line"></span><br><span class="line">tf.convert_to_tensor([<span class="number">1.</span>,<span class="number">2.</span>])</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=161, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span></span><br><span class="line"></span><br><span class="line">tf.convert_to_tensor([<span class="number">1</span>, <span class="number">2.</span>])</span><br><span class="line"><span class="comment">#&lt;tf.Tensor: id=179, shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)&gt;</span></span><br><span class="line"></span><br><span class="line">tf.convert_to_tensor([[<span class="number">1</span>],[<span class="number">2.</span>]])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=196, shape=(2, 1), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[1.],</span></span><br><span class="line"><span class="string">       [2.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错示例</span></span><br><span class="line">tf.convert_to_tensor([<span class="number">1</span>, (<span class="number">1</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="comment"># ValueError: Can't convert Python sequence with mixed types to Tensor.</span></span><br></pre></td></tr></table></figure>
<p>convert_to_tensor 把 参数 理解成 data</p>
<h4 id="zeros-ones"><a href="#zeros-ones" class="headerlink" title="zeros, ones"></a>zeros, ones</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">tf.zeros([]) <span class="comment"># scatter</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=0, shape=(), dtype=float32, numpy=0.0&gt;</span></span><br><span class="line"></span><br><span class="line">tf.zeros([<span class="number">1</span>]) <span class="comment"># vector</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=3, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)&gt;</span></span><br><span class="line"></span><br><span class="line">tf.zeros([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=6, shape=(2, 2), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[0., 0.],</span></span><br><span class="line"><span class="string">       [0., 0.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.zeros([<span class="number">2</span>,<span class="number">1</span>]) <span class="comment"># 2行1列</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=9, shape=(2, 1), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[0.],</span></span><br><span class="line"><span class="string">       [0.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.zeros([<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=12, shape=(2, 3, 3), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]],</span></span><br><span class="line"><span class="string">       [[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.ones([<span class="number">2</span>])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=15, shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<p>zeros 和 ones 把 参数 理解为 shape</p>
<h4 id="zeros-like"><a href="#zeros-like" class="headerlink" title="zeros_like"></a>zeros_like</h4><p>直接根据传入 Tensor ，创建一个 同 shape 但元素全为0 的 Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.zeros_like(a)</span><br><span class="line"><span class="comment"># 等同，两个API通用</span></span><br><span class="line">tf.zeros(a.shape)</span><br></pre></td></tr></table></figure>
<p>ones_like 同理</p>
<h4 id="Fill"><a href="#Fill" class="headerlink" title="Fill"></a>Fill</h4><p>给 shape 填充 相同的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.fill([<span class="number">2</span>,<span class="number">3</span>], <span class="number">0</span>) <span class="comment"># 等同于 tf.zeros([2,3])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=18, shape=(2, 3), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.fill([<span class="number">2</span>,<span class="number">3</span>], <span class="number">9</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=21, shape=(2, 3), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[9, 9, 9],</span></span><br><span class="line"><span class="string">       [9, 9, 9]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<blockquote>
<h5 id="tf-random-下的分布"><a href="#tf-random-下的分布" class="headerlink" title="tf.random 下的分布"></a>tf.random 下的分布</h5><ul>
<li>normal  |  正态分布</li>
<li>truncated_normal  |  截断的正态分布</li>
<li>uniform   |  均匀分布</li>
</ul>
</blockquote>
<h4 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h4><p>随机初始化，e.g.   w 和 b参数用正态分布或其他分布去初始化，一般 b 初始化为 0，w 可以初始化为 1，也可以随机。在DP中没这么简单，很多 paper 研究了不同的初始化方案对DP的性能，收敛速度都有显著影响。比如 xvier 有的较多，类似于某一种分布的初始化方案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.random.normal([<span class="number">2</span>,<span class="number">2</span>], mean=<span class="number">1</span>, stddev=<span class="number">1</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=27, shape=(2, 2), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[-0.29903114,  1.3298839 ],</span></span><br><span class="line"><span class="string">       [ 0.557529  ,  0.7930608 ]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.random.normal([<span class="number">2</span>,<span class="number">2</span>]) <span class="comment"># 默认是标准正态</span></span><br></pre></td></tr></table></figure>
<p><strong>truncated_normal 使用场景举例：</strong></p>
<p>在原有正态分布基础上，截去某一部分元素，如下图</p>
<p><img src="/images/truncated1.jpg" alt></p>
<p>把红色部分截去，在其他区域进行采样。</p>
<p>这样做的好处：在 Deep Learning 中，有 Sigmoid 激活函数 (类似生物学中S型曲线)，它有正态分布有交集，在横轴两侧，曲线区域梯度平缓，接近于0，这种现象称为 梯度弥散 or 梯度消失（Truncated Distribution）。这种现象出现时，更新会很困难。为了避免这种现象就引入了 truncated_normal ，性能比 normal 好一些。</p>
<h4 id="Uniform"><a href="#Uniform" class="headerlink" title="Uniform"></a>Uniform</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.random.uniform([<span class="number">2</span>,<span class="number">3</span>], minval = <span class="number">0</span>, maxval=<span class="number">100</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=34, shape=(2, 3), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[96.666954, 17.974722, 23.972082],</span></span><br><span class="line"><span class="string">       [62.272835, 95.62509 , 11.372995]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">tf.random.uniform([<span class="number">2</span>,<span class="number">3</span>], minval = <span class="number">0</span>, maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=38, shape=(2, 3), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[57, 79, 37],</span></span><br><span class="line"><span class="string">       [37, 23, 29]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="Random-Permutation"><a href="#Random-Permutation" class="headerlink" title="Random Permutation"></a>Random Permutation</h4><p>小应用：随机打散，常用于 打散有对应关系的两个数据。比如 features 和 label 有 一一对应 关系，对 features 打散的时候，需要对 label 按照同样的顺序打散。这就需要相同的  index。</p>
<p>[64, 28, 28, 3] ： 64张照片，每张照片[28, 28, 3]，目的就是把照片顺序打乱</p>
<p>生成 0-63 索引，随机打散形成新的序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">idx = tf.range(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=42, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;</span></span><br><span class="line"></span><br><span class="line">idx = tf.random.shuffle(idx)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=43, shape=(10,), dtype=int32, numpy=array([0, 3, 4, 6, 8, 2, 9, 7, 1, 5])&gt;</span></span><br><span class="line"></span><br><span class="line">a = tf.random.normal([<span class="number">10</span>,<span class="number">784</span>])</span><br><span class="line">b = tf.random.uniform([<span class="number">10</span>], maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=53, shape=(10,), dtype=int32, numpy=array([5, 5, 3, 5, 5, 2, 6, 5, 5, 1])&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用同样的 idx 从a和b中拿数据，这样就可以保证在经过随机后仍然保持对应关系</span></span><br><span class="line">a = tf.gather(a, idx)</span><br><span class="line">b = tf.gather(b, idx)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=57, shape=(10,), dtype=int32, numpy=array([5, 5, 5, 6, 5, 3, 1, 5, 5, 2])&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="constant"><a href="#constant" class="headerlink" title="constant"></a>constant</h4><p>跟 convert_to_tensor 基本上完全重合，参数理解为 data，但维度必须一样</p>
<ul>
<li>constant 意思是不能改变，名字来源于 1.x 的，1.x中代表常量，现在就把它理解成一个普通的 Tensor</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据，类型取决于给定的参数 或者 手动给定</span></span><br><span class="line">tf.constant(<span class="number">1</span>) <span class="comment"># int32</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=0, shape=(), dtype=int32, numpy=1&gt;</span></span><br><span class="line"></span><br><span class="line">tf.constant(<span class="number">1.</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=1, shape=(), dtype=float32, numpy=1.0&gt;</span></span><br><span class="line"></span><br><span class="line">tf.constant(<span class="number">2.</span>, dtype=tf.double)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=2, shape=(), dtype=float64, numpy=2.0&gt;</span></span><br><span class="line"></span><br><span class="line">tf.constant([<span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=3, shape=(2,), dtype=bool, numpy=array([ True, False])&gt;</span></span><br><span class="line"></span><br><span class="line">tf.constant(<span class="string">'hello,world.'</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=4, shape=(), dtype=string, numpy=b'hello,world.'&gt;</span></span><br></pre></td></tr></table></figure>
<p>错误示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.constant([[<span class="number">1</span>,<span class="number">2.</span>], [<span class="number">3.</span>]])</span><br><span class="line"><span class="comment"># ValueError: Can't convert non-rectangular Python sequence to Tensor.</span></span><br></pre></td></tr></table></figure>
<h4 id="Typecal-Dim-Data"><a href="#Typecal-Dim-Data" class="headerlink" title="Typecal Dim Data"></a>Typecal Dim Data</h4><p>不同 Tensor 的 shape 下的具体应用案例，帮助理解不同形状 Tensor</p>
<h5 id="Scalar-标量"><a href="#Scalar-标量" class="headerlink" title="Scalar |  标量"></a>Scalar |  标量</h5><ul>
<li>[]<ul>
<li>0</li>
<li>1.</li>
<li>2.2</li>
</ul>
</li>
<li>loss = mse(out, y)</li>
<li>accuracy</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比如4张照片，10类</span></span><br><span class="line"><span class="comment"># 这里out没有经过神经网络运算，用随机分布模拟</span></span><br><span class="line">out = tf.random.uniform([<span class="number">4</span>,<span class="number">10</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=6, shape=(4, 10), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[0.9096954 , 0.49205315, 0.62928593, 0.6149267 , 0.6901686 ,</span></span><br><span class="line"><span class="string">        0.04514229, 0.1058538 , 0.84283805, 0.8563384 , 0.11697829],</span></span><br><span class="line"><span class="string">       [0.5169194 , 0.02506113, 0.5753865 , 0.39724565, 0.3482896 ,</span></span><br><span class="line"><span class="string">        0.55755997, 0.43404877, 0.5104655 , 0.19031775, 0.98010445],</span></span><br><span class="line"><span class="string">       [0.82540715, 0.10581172, 0.32901287, 0.38353086, 0.48970842,</span></span><br><span class="line"><span class="string">        0.85302186, 0.4669423 , 0.75125694, 0.08376086, 0.9594767 ],</span></span><br><span class="line"><span class="string">       [0.9461738 , 0.12383306, 0.88509655, 0.54084754, 0.5327699 ,</span></span><br><span class="line"><span class="string">        0.54813695, 0.32469988, 0.58895886, 0.26172686, 0.5735899 ]],</span></span><br><span class="line"><span class="string">      dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定 label 为0,1,2,3</span></span><br><span class="line">y = tf.range(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=10, shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])&gt;</span></span><br><span class="line">y = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=14, shape=(4, 10), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mse: (y-out)^2 累加求和</span></span><br><span class="line">loss = tf.keras.losses.mse(y, out)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=17, shape=(4,), dtype=float32, numpy=array([0.29713577, 0.35845628, 0.39673066, 0.3338903 ], dtype=float32)&gt;</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(loss)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=19, shape=(), dtype=float32, numpy=0.34655327&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Vector-向量"><a href="#Vector-向量" class="headerlink" title="Vector | 向量"></a>Vector | 向量</h5><ul>
<li><p>Bias</p>
<ul>
<li>[out_dim]</li>
</ul>
<p>$y=x@w+b$，b 为 [10]，维度为1</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dense层 8维 转化为 10维，功能都是维度变化</span></span><br><span class="line">net = tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">net.build((<span class="number">4</span>,<span class="number">8</span>))</span><br><span class="line"><span class="comment"># kernel 初始化方法未知，可能是某种random</span></span><br><span class="line">net.kernel <span class="comment"># w 参数</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Variable 'kernel:0' shape=(8, 10) dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[-0.5167706 , -0.42759433,  0.27488554,  0.40619868, -0.16825691,</span></span><br><span class="line"><span class="string">        -0.53854436, -0.3289101 , -0.3599288 ,  0.312037  , -0.19269094],</span></span><br><span class="line"><span class="string">       [ 0.46941042,  0.4642514 ,  0.54453063,  0.34717393, -0.33969906,</span></span><br><span class="line"><span class="string">         0.20790648,  0.00850588, -0.51672363,  0.27600366,  0.45641685],</span></span><br><span class="line"><span class="string">       [ 0.34303325, -0.11325005, -0.21864352,  0.48297668, -0.12095174,</span></span><br><span class="line"><span class="string">         0.09572101, -0.40715218, -0.35717124, -0.20483121, -0.46552092],</span></span><br><span class="line"><span class="string">       [ 0.4865899 ,  0.05136955, -0.09178695,  0.17259252,  0.28631455,</span></span><br><span class="line"><span class="string">         0.2660445 , -0.41543508,  0.05910832, -0.19016105, -0.47269845],</span></span><br><span class="line"><span class="string">       [ 0.00252616, -0.12237576, -0.38564754,  0.1557622 , -0.49814075,</span></span><br><span class="line"><span class="string">         0.10102415,  0.08350313,  0.06122816,  0.30136532,  0.5090873 ],</span></span><br><span class="line"><span class="string">       [-0.14992028, -0.40108496,  0.32735962, -0.15980184,  0.16353315,</span></span><br><span class="line"><span class="string">         0.37525266, -0.35841423, -0.47843078,  0.35405552, -0.3584773 ],</span></span><br><span class="line"><span class="string">       [ 0.32667726,  0.40449303, -0.4557876 ,  0.44946396,  0.47827542,</span></span><br><span class="line"><span class="string">        -0.04340398,  0.49171865,  0.4222148 ,  0.11599368, -0.13451478],</span></span><br><span class="line"><span class="string">       [-0.28062007,  0.40846568, -0.3182774 ,  0.5397973 , -0.12705025,</span></span><br><span class="line"><span class="string">        -0.12112314, -0.49585712,  0.26155084,  0.11145604,  0.38149667]],</span></span><br><span class="line"><span class="string">      dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># Dense layer初始化功能：将bias 这个Tensor 初始化为0</span></span><br><span class="line">net.bias <span class="comment"># b 参数</span></span><br><span class="line"><span class="comment"># &lt;tf.Variable 'bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="Matrix-矩阵"><a href="#Matrix-矩阵" class="headerlink" title="Matrix | 矩阵"></a>Matrix | 矩阵</h5><ul>
<li><p>input x: [b, vec_dim]</p>
<p>多张照片 [b, 784]</p>
</li>
<li><p>weight: [input_dim, output_dim]</p>
<p>比如上述例子的 w [8, 10]</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4张照片，每张照片是28*28打平</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">784</span>])</span><br><span class="line">x.shape <span class="comment"># TensorShape([4, 784])</span></span><br><span class="line"><span class="comment"># Dense层，维度为10，即 784-&gt;10 的过程</span></span><br><span class="line">net = tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">net.build((<span class="number">4</span>,<span class="number">784</span>))</span><br><span class="line"><span class="comment"># 将x送到网络后，得到 [4,10]: 4张照片，每张照片输出shape是10</span></span><br><span class="line">net(x).shape <span class="comment"># TensorShape([4, 10])</span></span><br><span class="line"><span class="comment"># w 参数</span></span><br><span class="line">net.kernel.shape <span class="comment"># TensorShape([784, 10])</span></span><br><span class="line"><span class="comment"># b 参数</span></span><br><span class="line">net.bias.shape <span class="comment"># TensorShape([10])</span></span><br></pre></td></tr></table></figure>
<h5 id="Dim-3-Tensor"><a href="#Dim-3-Tensor" class="headerlink" title="Dim=3 Tensor"></a>Dim=3 Tensor</h5><p>在 NLP 方面用途广泛，NLP 中把每个单词转化为一个 向量</p>
<ul>
<li><p>x: [b, seq_len, word_dim]</p>
<ul>
<li>b |  句子个数</li>
<li>seq_len |  每个句子单词数量</li>
<li>word_dim |  单词编码长度</li>
</ul>
</li>
<li><p>I am a student. 可编码为 Tensor: [1, 5, 5] </p>
<p><img src="/images/NLP1.jpg" alt></p>
</li>
</ul>
<blockquote>
<p>imdb 电影评论数据，比如对xx的评价，有80个单词，label是好评or差评</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># imdb 电影评论数据</span></span><br><span class="line">(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz</span></span><br><span class="line">x_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=<span class="number">80</span>)</span><br><span class="line">x_train.shape <span class="comment"># (25000, 80)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码</span></span><br><span class="line">emb = embedding(x_train)</span><br><span class="line">emb.shape <span class="comment"># TensorShape([25000, 80, 100])  每个单词100维向量编码生成</span></span><br><span class="line"></span><br><span class="line">out = rnn(emb[:<span class="number">4</span>])</span><br><span class="line">out.shape <span class="comment"># TensorShape()</span></span><br></pre></td></tr></table></figure>
<h5 id="Dim-4-Tensor"><a href="#Dim-4-Tensor" class="headerlink" title="Dim=4 Tensor"></a>Dim=4 Tensor</h5><p>图片的一种保存方式，需要对几个简写非常敏感，做卷积神经网络时候用的特别多</p>
<ul>
<li>Image: [b, h, w, 3]<ul>
<li>MNIST: [b, 28, 28, 1]</li>
</ul>
</li>
<li>feature maps: [b, h, w, c]<ul>
<li>b  | 图片数量</li>
<li>h  |  height</li>
<li>w  | width</li>
<li>c  |  channel , RGB为3</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模拟4张照片</span></span><br><span class="line">x = tf.random.normal((<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>)) <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line">net = tf.keras.layers.Conv2D(<span class="number">16</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">net(x) <span class="comment"># [4, 32, 32, 16]</span></span><br><span class="line"><span class="comment"># tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># channel 3 -&gt; 16</span></span><br></pre></td></tr></table></figure>
<h5 id="Dim-5-Tensor"><a href="#Dim-5-Tensor" class="headerlink" title="Dim=5 Tensor"></a>Dim=5 Tensor</h5><p>dim大于4的这种情况较少，dim=5 主要用于 meta-learning：把数据集分割为多个任务</p>
<ul>
<li><p>Single Task: [b, h, w, 3]</p>
</li>
<li><p>[t, b, h, w, c]</p>
<ul>
<li>t  |  任务数</li>
<li>[b, h, w, c]  |  Single Task</li>
</ul>
<p>训练时，比普通的卷积层多了新的维度（任务维度）</p>
</li>
</ul>
<p>维度再高的话比较少见</p>
<p>对于 Tensor 的理解：只要掌握了每一个维度的具体含义，就知道了Tensor代表什么类型数据存储，进行变换时，紧紧跟踪住每一个维度具体含义，这样就知道了这个维度做了何种运算，运算的具体含义是什么。</p>
<h3 id="Tensor-Property"><a href="#Tensor-Property" class="headerlink" title="Tensor Property"></a>Tensor Property</h3><ul>
<li>device</li>
<li>cpu()</li>
<li>gpu()</li>
<li>numpy()</li>
<li>shape</li>
<li>ndim</li>
<li>dtype | 数据类型</li>
<li><del>name</del> | 1.x遗留问题，并不需要这个属性</li>
<li>trainable |  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对应环境下创建 Tensor</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'cpu'</span>):</span><br><span class="line">    a=tf.constant([<span class="number">1</span>])</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'gpu'</span>):</span><br><span class="line">    b=tf.range(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 查看 Tensor 所在设备的名称</span></span><br><span class="line">a.device <span class="comment"># '/job:localhost/replica:0/task:0/device:CPU:0'</span></span><br><span class="line">b.device <span class="comment"># '/job:localhost/replica:0/task:0/device:GPU:0'</span></span><br><span class="line"><span class="comment"># 切换设备，返回新的Tensor</span></span><br><span class="line">aa = a.gpu()</span><br><span class="line">aa.device <span class="comment"># '/job:localhost/replica:0/task:0/device:GPU:0'</span></span><br><span class="line"></span><br><span class="line">bb = b.cpu()</span><br><span class="line">bb.device <span class="comment"># '/job:localhost/replica:0/task:0/device:CPU:0'</span></span><br><span class="line"></span><br><span class="line">bnumpy = b.numpy() <span class="comment"># array([0, 1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">b.shape <span class="comment"># TensorShape([4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一般用 ndim 看维度</span></span><br><span class="line">b.ndim <span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line">tf.rank(b) <span class="comment"># &lt;tf.Tensor: id=12, shape=(), dtype=int32, numpy=1&gt;</span></span><br><span class="line"></span><br><span class="line">tf.rank(tf.ones([<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>])) <span class="comment"># &lt;tf.Tensor: id=16, shape=(), dtype=int32, numpy=3&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>CPU 和 GPU上的 Tensor 区别：对于GPU上的Tensor，只能使用 GPU 上的操作，但操作往往是自动判定的，操作符根据传入参数的 Tensor 进行自动选择 在对应设备 上进行操作，如果 Tensor 的设备不一致就会报错，需要将设备调整到一致。</p>
</blockquote>
<h3 id="Check-Tensor-Type"><a href="#Check-Tensor-Type" class="headerlink" title="Check Tensor Type"></a>Check Tensor Type</h3><p>使用场景：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(a)</span>:</span></span><br><span class="line">    <span class="comment"># 先判断是不是Tensor，如果不是需要转化</span></span><br><span class="line">    <span class="comment"># 判断类型</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h4 id="判断一个变量是不是-Tensor"><a href="#判断一个变量是不是-Tensor" class="headerlink" title="判断一个变量是不是 Tensor"></a>判断一个变量是不是 Tensor</h4><ul>
<li>isinstance(var, tf.Tensor)<ul>
<li>Variable 的时候会返回 False</li>
</ul>
</li>
<li>tf.is_tensor(var) || 推荐使用！</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">isinstance(a, tf.Tensor) <span class="comment"># True</span></span><br><span class="line">tf.is_tensor(b) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h4 id="判断-Tensor-类型"><a href="#判断-Tensor-类型" class="headerlink" title="判断 Tensor 类型"></a>判断 Tensor 类型</h4><ul>
<li>tf.float32</li>
<li>tf.string</li>
<li>tf.bool</li>
<li>……</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.dtype == tf.int32 <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<h4 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h4><ul>
<li>tf.convert_to_tensor(var, dtype) | numpy —&gt; Tensor</li>
<li>tf.cast(var, dtype)  |  Tensor 之间</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">5</span>) <span class="comment"># array([0, 1, 2, 3, 4])</span></span><br><span class="line">a.dtype <span class="comment"># dtype('int32')</span></span><br><span class="line">aa = tf.convert_to_tensor(a)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=0, shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;</span></span><br><span class="line"></span><br><span class="line">aa = tf.convert_to_tensor(a, dtype=tf.int64)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=1, shape=(5,), dtype=int64, numpy=array([0, 1, 2, 3, 4], dtype=int64)&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bool类型转化很常见</span></span><br><span class="line">b = tf.constant([<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=2, shape=(2,), dtype=int32, numpy=array([0, 1])&gt;</span></span><br><span class="line">bb = tf.cast(b, dtype=tf.bool)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=3, shape=(2,), dtype=bool, numpy=array([False,  True])&gt;</span></span><br><span class="line">tf.cast(bb, tf.int32)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=4, shape=(2,), dtype=int32, numpy=array([0, 1])&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="tf-Variable"><a href="#tf-Variable" class="headerlink" title="tf.Variable"></a>tf.Variable</h3><p>特殊的数据类型，专门对于可以优化的参数</p>
<p>eg: $y=wx+b$,  x 和 y 都是 Tensor 类型， w 和 b 都是需要被优化的参数，除了是 Tensor 类型外，还有一个额外的属性 Variable </p>
<p>Tensor 经过 Variable 包装后，自动就具有了可求导的特性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = tf.range(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=3, shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;</span></span><br><span class="line"></span><br><span class="line">b = tf.Variable(a)</span><br><span class="line"><span class="comment"># &lt;tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([0, 1, 2, 3, 4])&gt;</span></span><br><span class="line"></span><br><span class="line">b.name <span class="comment"># 'Variable:0'</span></span><br><span class="line">b.trainable <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">isinstance(b, tf.Tensor) <span class="comment"># False</span></span><br><span class="line">isinstance(b, tf.Variable) <span class="comment"># True</span></span><br><span class="line">tf.is_tensor(b) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>trainable 是 告诉 网络/计算图 进行 先后传播/求导 需要对 b 这个参数 进行求导，需要梯度信息，即$\frac{\partial loss}{\part w}$ 这样一个过程，如果不转化的话也可以，但需要额外指定 这张图 需要对这个 Tensor 观察，不然它不会对该 Tensor 不会进行跟梯度信息相关的跟踪（后续会实例讲解哪些需要 watch，哪些需要转化为 Variable）。</p>
<p>当转化为 Variable 后会自动记录 梯度相关信息，该类型专为神经网络参数设计，是对Tensor的一个包装。</p>
<h3 id="To-numpy"><a href="#To-numpy" class="headerlink" title="To numpy"></a>To numpy</h3><p>Tensor 数据转化为 numpy</p>
<p>在运行中间，Tensor 一般是在GPU上，当需要在CPU上进行一些额外的控制逻辑时，需要把数据取回到CPU上，得到具体的数据（调用 numpy 方法）</p>
<p>对于标量型的，还有更简单的，直接用 int(), float() 即可，前提是该 Tensor 是 Scatter</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([])</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=85, shape=(), dtype=float32, numpy=1.0&gt;</span></span><br><span class="line">a.numpy() <span class="comment"># 1.0</span></span><br><span class="line">int(a) <span class="comment"># 1</span></span><br><span class="line">float(a) <span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>
<h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><p>读取 Tensor 单个数据，每个维度指定一个数值</p>
<h4 id="Basic-indexing"><a href="#Basic-indexing" class="headerlink" title="Basic indexing"></a>Basic indexing</h4><p>最基本的索引方式，逐渐给出外层的 idx</p>
<ul>
<li>支持索引方式单一</li>
<li>需要写多个 []，可读性不强</li>
<li>只能取 1 个元素，不能隔断取，倒着取</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic indexing</span></span><br><span class="line">a = tf.ones([<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">3</span>]) <span class="comment"># TensorShape([1, 5, 5, 3])</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=10, shape=(5, 3), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=22, shape=(3,), dtype=float32, numpy=array([1., 1., 1.], dtype=float32)&gt;</span></span><br><span class="line">a[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">2</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=38, shape=(), dtype=float32, numpy=1.0&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="NumPy-style-indexing"><a href="#NumPy-style-indexing" class="headerlink" title="NumPy-style indexing"></a>NumPy-style indexing</h4><p>NumPy 对索引方式进行了很好的拓展：所有编号写一起，中间用逗号隔开</p>
<ul>
<li>少写很多 []</li>
<li>程序可读性强</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NumPy-style indexing</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>]) <span class="comment"># 可理解成 4张28*28的彩色照片</span></span><br><span class="line">a[<span class="number">1</span>].shape <span class="comment"># TensorShape([28, 28, 3])  第2张照片</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>].shape <span class="comment"># 同 a[1][2]，但不推荐</span></span><br><span class="line"><span class="comment"># TensorShape([28, 3]) 第2张照片第3行：每一行包含了28列，每一列有3个RGB数值</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>].shape</span><br><span class="line"><span class="comment"># TensorShape([3]) 第2张照片第3行第4列：具体的RGB数值</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> &lt;tf.Tensor: id=74, shape=(3,), dtype=float32, numpy=array([ 0.11981988,  0.94879496, -0.42506   ], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>].shape</span><br><span class="line"><span class="comment"># TensorShape([]) 第2张照片第3行第4列 B通道：蓝色通道色素的值(0-255或归一化)</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=94, shape=(), dtype=float32, numpy=-0.42506&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h3><p>读取 Tensor 某维度的一部分</p>
<h4 id="start-end"><a href="#start-end" class="headerlink" title="start:end"></a>start:end</h4><p>包含 start, 不含索引 end。默认为 </p>
<ul>
<li>[:]  =&gt;  所有元素都取</li>
<li>start 默认为0开始，包含</li>
<li>end 如果不指定 则 取到末尾，如果指定，取到 end-1 位置结束</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.range(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#  &lt;tf.Tensor: id=3, shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意细微区别</span></span><br><span class="line">a[<span class="number">-1</span>:] <span class="comment"># &lt;tf.Tensor: id=7, shape=(1,), dtype=int32, numpy=array([9])&gt;</span></span><br><span class="line">a[<span class="number">-1</span>]  <span class="comment"># &lt;tf.Tensor: id=15, shape=(), dtype=int32, numpy=9&gt;</span></span><br><span class="line">a[<span class="number">-2</span>:] <span class="comment"># &lt;tf.Tensor: id=11, shape=(2,), dtype=int32, numpy=array([8, 9])&gt;</span></span><br><span class="line">a[:<span class="number">2</span>]  <span class="comment"># &lt;tf.Tensor: id=23, shape=(2,), dtype=int32, numpy=array([0, 1])&gt;</span></span><br><span class="line">a[:<span class="number">-1</span>] <span class="comment"># &lt;tf.Tensor: id=19, shape=(9,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8])&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>]) <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line">a[<span class="number">0</span>].shape <span class="comment"># TensorShape([28, 28, 3])</span></span><br><span class="line"><span class="comment"># a[0]等价写法： a[0,:,:,:]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取所有图片，所有行所有列的 R 通道，这种情况不使用切片无法完成</span></span><br><span class="line">a[:,:,:,<span class="number">0</span>].shape <span class="comment"># TensorShape([4, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可对中间某维度进行切片，之后维度全部都取可以不写</span></span><br><span class="line">a[:,<span class="number">0</span>:<span class="number">2</span>].shape <span class="comment"># TensorShape([4, 2, 28, 3])</span></span><br><span class="line">a[:,<span class="number">0</span>:<span class="number">2</span>,:].shape <span class="comment"># TensorShape([4, 2, 28, 3]) </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取所有图片，所有行所有列的 Blue 通道</span></span><br><span class="line">a[:,:,:,<span class="number">2</span>].shape <span class="comment"># TensorShape([4, 28, 28])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4张图片 第1行 所有列所有通道</span></span><br><span class="line">a[:,<span class="number">0</span>,:,:].shape <span class="comment"># TensorShape([4, 28, 3])</span></span><br></pre></td></tr></table></figure>
<p>冒号这种索引方式会很丰富</p>
<h4 id="start-end-step"><a href="#start-end-step" class="headerlink" title="start: end: step"></a>start: end: step</h4><p>在上一小节基础上，加入步长的概念，step省略的情况（step为1）下回到了单冒号的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>]) <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line">a.shape <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line">a[<span class="number">0</span>:<span class="number">2</span>,:,:,:].shape <span class="comment"># TensorShape([2, 28, 28, 3])</span></span><br><span class="line">a[:,<span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">28</span>:<span class="number">2</span>,:].shape <span class="comment"># TensorShape([4, 14, 14, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取图像的左上方</span></span><br><span class="line">a[:,:<span class="number">14</span>,:<span class="number">14</span>,:].shape <span class="comment"># TensorShape([4, 14, 14, 3])</span></span><br><span class="line"><span class="comment"># 取图像的右下方</span></span><br><span class="line">a[:,<span class="number">14</span>:,<span class="number">14</span>:,:].shape <span class="comment"># TensorShape([4, 14, 14, 3])</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">a[:,::<span class="number">2</span>,::<span class="number">2</span>,:].shape <span class="comment"># TensorShape([4, 14, 14, 3])</span></span><br><span class="line"><span class="comment"># 去除边缘，对中间部分进行采样</span></span><br><span class="line">a[:,<span class="number">2</span>:<span class="number">26</span>:<span class="number">2</span>,<span class="number">2</span>:<span class="number">26</span>:<span class="number">2</span>,:].shape <span class="comment"># TensorShape([4, 12, 12, 3])</span></span><br></pre></td></tr></table></figure>
<h4 id="1"><a href="#1" class="headerlink" title="::-1"></a>::-1</h4><p>逆序功能</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.range(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=3, shape=(4,), dtype=int32, numpy=array([0, 1, 2, 3])&gt;</span></span><br><span class="line">a[::<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=7, shape=(4,), dtype=int32, numpy=array([3, 2, 1, 0])&gt;</span></span><br><span class="line">a[::<span class="number">-2</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=11, shape=(2,), dtype=int32, numpy=array([3, 1])&gt;</span></span><br><span class="line">a[<span class="number">2</span>::<span class="number">-2</span>]</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=15, shape=(2,), dtype=int32, numpy=array([2, 0])&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="…"><a href="#…" class="headerlink" title="…"></a>…</h4><p>…表示其所代表的维度都取，只要能从逻辑上推断出 … 代表哪些维度，就可以使用</p>
<p>[x,…,y]，y只能代表最后的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加task维度</span></span><br><span class="line">a = tf.random.normal([<span class="number">2</span>,<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 等价写法1, 以下均为 TensorShape([4, 28, 28, 3])</span></span><br><span class="line">a[<span class="number">0</span>].shape</span><br><span class="line">a[<span class="number">0</span>,:,:,:,:].shape</span><br><span class="line">a[<span class="number">0</span>,::,::,::,::].shape</span><br><span class="line"><span class="comment"># 引入省略号实现等价写法1</span></span><br><span class="line">a[<span class="number">0</span>,...].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价写法2, TensorShape([2, 4, 28, 28])</span></span><br><span class="line">a[:,:,:,:,<span class="number">0</span>].shape</span><br><span class="line">a[...,<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价写法3, TensorShape([4, 28, 28])</span></span><br><span class="line">a[<span class="number">0</span>,:,:,:,<span class="number">2</span>].shape</span><br><span class="line">a[<span class="number">0</span>,...,<span class="number">2</span>].shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价写法4, TensorShape([28, 28])</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">0</span>,:,:,<span class="number">0</span>].shape</span><br><span class="line">a[<span class="number">1</span>,<span class="number">0</span>,...,<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure>
<h3 id="Selective-Indexing"><a href="#Selective-Indexing" class="headerlink" title="Selective Indexing"></a>Selective Indexing</h3><p>无上述所述的规律，随意采样。顺序由自己定！灵活性很强</p>
<h4 id="tf-gather"><a href="#tf-gather" class="headerlink" title="tf.gather"></a>tf.gather</h4><p>收集功能：给定 索引 收集对应维度上 索引所在的的元素（单维度上可以DIY）</p>
<ul>
<li>data: [classes, students, subjects]  e.g. [4, 35, 8]</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>], maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line"><span class="comment"># 以下两种写法等价</span></span><br><span class="line">a[<span class="number">2</span>:<span class="number">4</span>].shape <span class="comment"># TensorShape([2, 35, 8])</span></span><br><span class="line">tf.gather(a, axis=<span class="number">0</span>, indices=[<span class="number">2</span>,<span class="number">3</span>]).shape <span class="comment"># TensorShape([2, 35, 8])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">a 为数据源</span></span><br><span class="line"><span class="string">axis=0 表示 取 classes 这个维度</span></span><br><span class="line"><span class="string">indices 给出取该维度具体的索引号</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 打散4个班的顺序</span></span><br><span class="line">tf.gather(a, axis=<span class="number">0</span>, indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>]).shape <span class="comment"># TensorShape([4, 35, 8])</span></span><br><span class="line"><span class="comment"># 对每个班抽样检查,抽取 指定索引学生(index顺序自己定) 的所有课程成绩</span></span><br><span class="line">tf.gather(a, axis=<span class="number">1</span>, indices=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">16</span>]).shape <span class="comment"># TensorShape([4, 5, 8])</span></span><br><span class="line"><span class="comment"># 查看所有班级 所有学生 指定科目的成绩</span></span><br><span class="line">tf.gather(a, axis=<span class="number">2</span>, indices=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>]).shape <span class="comment"># TensorShape([4, 35, 3])</span></span><br></pre></td></tr></table></figure>
<h4 id="tf-gather-nd"><a href="#tf-gather-nd" class="headerlink" title="tf.gather_nd"></a>tf.gather_nd</h4><p>What if sample several students and their several subjects?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 串行通过两个 gather 实现</span></span><br><span class="line">aa = tf.gather(a, axis, [several students])</span><br><span class="line">aaa = tf.gather(aa, axis, [several subjects])</span><br></pre></td></tr></table></figure>
<p>What if sample several (classes and student)?  </p>
<ul>
<li>e.g. 2—0,  3—4,  8—2</li>
<li>[class1_student1, class2_student2, class3_student3, class4_student4] ,其shape [4,8]</li>
</ul>
<p>上述需求需要 同时在多个维度上 指定 idx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>], maxval=<span class="number">100</span>, dtype=tf.int32)</span><br><span class="line">a.shape <span class="comment"># TensorShape([4, 35, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0号班级所有成绩</span></span><br><span class="line">tf.gather_nd(a, [<span class="number">0</span>]).shape <span class="comment"># TensorShape([35, 8]), 相当于 a[0]</span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>]]).shape <span class="comment"># TensorShape([1, 35, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0号班级 1号学生 所有成绩</span></span><br><span class="line">tf.gather_nd(a, [<span class="number">0</span>,<span class="number">1</span>]).shape <span class="comment"># TensorShape([8]), 相当于a[0,1]</span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>,<span class="number">1</span>]]).shape <span class="comment"># TensorShape([1, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0号班级 1号学生 2号课程 的成绩</span></span><br><span class="line">tf.gather_nd(a, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]).shape <span class="comment"># TensorShape([])</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=33, shape=(), dtype=int32, numpy=11&gt;</span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]]).shape <span class="comment"># TensorShape([1])</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=31, shape=(1,), dtype=int32, numpy=array([11])&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>]]).shape <span class="comment"># TensorShape([2, 8])</span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>,<span class="number">0</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">2</span>,<span class="number">2</span>]]).shape <span class="comment"># TensorShape([3, 8])</span></span><br><span class="line">tf.gather_nd(a, [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]).shape <span class="comment"># TensorShape([3])</span></span><br><span class="line">tf.gather_nd(a, [[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>]]]).shape <span class="comment"># TensorShape([1, 3])</span></span><br></pre></td></tr></table></figure>
<ul>
<li>最内层作为整体来看</li>
<li>recommend indices format:<ul>
<li>[[0], [1], …]</li>
<li>[[0,0], [1,1], …]</li>
<li>[[0,0,0], [1,1,1], …]</li>
</ul>
</li>
</ul>
<p>如果理解上有困难可以跳过，部分关键代码会用到</p>
<h4 id="tf-boolean-mask"><a href="#tf-boolean-mask" class="headerlink" title="tf.boolean_mask"></a>tf.boolean_mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>]) <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line">tf.boolean_mask(a, mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>]).shape <span class="comment"># TensorShape([2, 28, 28, 3])</span></span><br><span class="line">tf.boolean_mask(a, mask=[<span class="literal">True</span>,<span class="literal">True</span>, <span class="literal">False</span>], axis=<span class="number">3</span>).shape <span class="comment"># TensorShape([4, 28, 28, 2])</span></span><br><span class="line"></span><br><span class="line">a = tf.ones([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">tf.boolean_mask(a, mask=[[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span>]])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=93, shape=(3, 4), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">       [1., 1., 1., 1.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 这里是 3是匹配到了多少个True</span></span><br></pre></td></tr></table></figure>
<p>只给出一个维度的情况：mask 为True时取出来</p>
<h3 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h3><p>[b, 28, 28, 1] 中，每一个数字代表一个轴(axis)，这个 Tensor 维度是 4维。数字本身代表 长度，也就是每个axis的shape。我们可以做一些不同理解上的变换（reshape）</p>
<p>另外还有 增加/减少 1个维度；对维度的先后顺序进行变换</p>
<h4 id="View-amp-Reshape"><a href="#View-amp-Reshape" class="headerlink" title="View &amp; Reshape"></a>View &amp; Reshape</h4><p>视图的概念：</p>
<p>[b, 28, 28] 中的照片在内存中先保存第1行，第2行……行优先，列为次。对于这一组内存中连续的数据可以作不同的理解</p>
<ul>
<li>每一张图片，每个图片28行，每一行28列，跟 content 理解一致</li>
<li>[b, 28*28] : 把28*28理解为整体，失去行列信息<ul>
<li>全连接层的使用方式：不关注二维信息</li>
</ul>
</li>
<li>[b, 2, 14*28] : 把图片分成上下两部分，每部分不区分行列</li>
<li>[b, 28, 28, 1] : 增加 channel，跟view1 差不多 </li>
</ul>
<p>对数据没有破坏/扰动，只不过按照不同的方式去理解，每种理解方式成为一个 view，具体物理意义取决于你。content 在这个过程中保持不变，就是最初的 view</p>
<p>reshape 操作是灵活的，保证size相同即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line">a.shape <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line">a.ndim <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把长和宽概念抹去，合并为像素(pixel)的概念</span></span><br><span class="line">tf.reshape(a, [<span class="number">4</span>,<span class="number">784</span>,<span class="number">3</span>]).shape <span class="comment"># TensorShape([4, 784, 3])</span></span><br><span class="line"><span class="comment"># 一个新的view 只能有一个 -1，-1位置会根据其他参数自动计算合理的值</span></span><br><span class="line">tf.reshape(a, [<span class="number">4</span>,<span class="number">-1</span>,<span class="number">3</span>]).shape <span class="comment"># TensorShape([4, 784, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pixel 和 channel 合并，物理含义：data point</span></span><br><span class="line"><span class="comment"># 无法恢复 channel 信息</span></span><br><span class="line">tf.reshape(a, [<span class="number">4</span>,<span class="number">784</span>*<span class="number">3</span>]).shape <span class="comment"># TensorShape([4, 2352])</span></span><br><span class="line">tf.reshape(a, [<span class="number">4</span>,<span class="number">-1</span>]).shape <span class="comment"># TensorShape([4, 2352])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># flexible</span></span><br><span class="line">tf.reshape(tf.reshape(a,[<span class="number">4</span>,<span class="number">-1</span>]),[<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>]).shape</span><br><span class="line">tf.reshape(tf.reshape(a,[<span class="number">4</span>,<span class="number">-1</span>]),[<span class="number">4</span>,<span class="number">14</span>,<span class="number">56</span>,<span class="number">3</span>]).shape</span><br><span class="line">tf.reshape(tf.reshape(a,[<span class="number">4</span>,<span class="number">-1</span>]),[<span class="number">4</span>,<span class="number">1</span>,<span class="number">784</span>,<span class="number">3</span>]).shape</span><br></pre></td></tr></table></figure>
<p>在数据在内存中的分布不变的情况下，只要你觉得reshape操作有含义，满足目的即可。</p>
<p>潜在的bug:</p>
<ul>
<li>[4,28,28,3] reshape to [4,784,3] 的时候，需要额外知道 height:28, width:28 的信息(content)才能恢复出来。</li>
<li>14*56</li>
<li>width 和 height 顺序记错</li>
</ul>
<h4 id="content-amp-transpose"><a href="#content-amp-transpose" class="headerlink" title="content &amp; transpose"></a>content &amp; transpose</h4><p>如果想把 维度 axis 进行交换（交换 content 顺序），可通过<code>tf.transpose</code> 实现。如 [b,h,w,c] 转为 [b,w,h,c]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal((<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">a.shape <span class="comment"># TensorShape([4, 3, 2, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认的转置方式</span></span><br><span class="line">tf.transpose(a).shape <span class="comment"># TensorShape([1, 2, 3, 4])</span></span><br><span class="line"><span class="comment"># 指定参数</span></span><br><span class="line">tf.transpose(a, perm=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]).shape <span class="comment"># TensorShape([4, 3, 1, 2])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">小实例：pytorch 中 存储格式 为 [b,3,h,w] , 而 TensorFlow 中为 [b,h,w,c]。如果要进行数据互通，需要把数据存储格式content转换</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># [b,h,w,c]</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># [b,w,h,c]</span></span><br><span class="line">tf.transpose(a, [<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>]).shape <span class="comment"># TensorShape([4, 28, 28, 3])</span></span><br><span class="line"><span class="comment"># pytorch的格式，但被转置了</span></span><br><span class="line">tf.transpose(a, [<span class="number">0</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]).shape <span class="comment"># TensorShape([4, 3, 28, 28])</span></span><br><span class="line"><span class="comment"># 正确做法</span></span><br><span class="line">tf.transpose(a, [<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>]).shape <span class="comment"># TensorShape([4, 3, 28, 28])</span></span><br></pre></td></tr></table></figure>
<h4 id="Squeeze-VS-Expand-dims"><a href="#Squeeze-VS-Expand-dims" class="headerlink" title="Squeeze VS Expand_dims"></a>Squeeze VS Expand_dims</h4><p>增加或减少维度，本质上也是改变view</p>
<h5 id="Expand-dim"><a href="#Expand-dim" class="headerlink" title="Expand dim"></a>Expand dim</h5><ul>
<li>a: [classes, students, classes]</li>
<li>add school dim(=axis)</li>
<li>[1,4,35,8] + [1,4,35,8] = [2,4,35,8]  (数据合并)</li>
<li><code>tf.expand_dims</code> 增加位置规律：<ul>
<li>当 <code>axis</code> 非负数，增加后的维度 在此位置，即加到 原来这个axis的前面</li>
<li>当 <code>axis</code> 为负数，加到 原来位置 的后面</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">0</span>).shape <span class="comment"># TensorShape([1, 4, 35, 8])</span></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">-4</span>).shape <span class="comment"># TensorShape([1, 4, 35, 8])</span></span><br><span class="line"></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">3</span>).shape <span class="comment"># TensorShape([4, 35, 8, 1])</span></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">-1</span>).shape <span class="comment"># TensorShape([4, 35, 8, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下仅供示范</span></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">1</span>).shape  <span class="comment"># TensorShape([4, 1, 35, 8])</span></span><br><span class="line">tf.expand_dims(a, axis=<span class="number">-2</span>).shape <span class="comment"># TensorShape([4, 35, 1, 8])</span></span><br></pre></td></tr></table></figure>
<h5 id="Squeeze-dim"><a href="#Squeeze-dim" class="headerlink" title="Squeeze dim"></a>Squeeze dim</h5><ul>
<li>Only squeeze for shape=1 dim<ul>
<li>[4, 35, 8, <font color="red">1</font>]</li>
<li>[<font color="red">1</font>, 4, 35, 8]</li>
<li>[<font color="red">1</font>, 4, 35, <font color="red">1</font>]</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于所有shape=1 的 axis 可以全部去掉(squeeze)</span></span><br><span class="line">tf.squeeze(tf.zeros([<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>])).shape <span class="comment"># TensorShape([2, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 axis 进行擦除dim</span></span><br><span class="line">a = tf.zeros([<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">tf.squeeze(a, axis=<span class="number">0</span>).shape <span class="comment"># TensorShape([2, 1, 3])</span></span><br><span class="line">tf.squeeze(a, axis=<span class="number">-4</span>).shape <span class="comment"># TensorShape([2, 1, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 并不存在 在前/在后 擦除的 区别，axis必须存在</span></span><br><span class="line">tf.squeeze(a, axis=<span class="number">2</span>).shape <span class="comment"># TensorShape([1, 2, 3])</span></span><br><span class="line">tf.squeeze(a, axis=<span class="number">-2</span>).shape <span class="comment"># TensorShape([1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><blockquote>
<p>背景：$x@w+b$ 在shape上体现就是 [b,784]@[784,10] + [10]；最后一步 + [10] 需要扩张 [b,10]才可以</p>
</blockquote>
<ul>
<li>Expand<ul>
<li><code>tf.broadcast_to</code> 张量的维度扩张的手段，对某一个维度上重复n多次，但并没有真正的复制数据<ul>
<li>可理解为优化的手段，没有复制数据，但呈现出来数据被扩张了。</li>
</ul>
</li>
<li><code>tf.tile</code> 是显式复制数据，对某个维度重复n多次，并且真实的在数据上体现出来</li>
</ul>
</li>
<li>Key idea<ul>
<li>Insert 1 dim ahead if needed</li>
<li>Expand dims with size 1 to same size</li>
</ul>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For example</span></span><br><span class="line"><span class="string">Feature</span> <span class="attr">maps:</span> <span class="string">[4,32,32,3]</span></span><br><span class="line"><span class="attr">Bias:</span> <span class="string">[3]-&gt;</span> <span class="string">[1,1,1,32]</span> <span class="bullet">-&gt;</span> <span class="string">[4,32,32,3]</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/Broadcasting-1.jpg" alt></p>
<p>[4] + [1,3] 右边对齐，插入维度后是 [1,4] vs. [1,3] , 3 和 4 不相等，两者之中有1的话可以扩张成相同的维度。无法进行 broadcasting</p>
<h4 id="How-to-understand"><a href="#How-to-understand" class="headerlink" title="How to understand?"></a>How to understand?</h4><ul>
<li><p>When it has no axis</p>
<ul>
<li>Create a new concept</li>
<li>[classes, students, scores] + [scores]</li>
</ul>
<p>理解：维度有大小之分，大维度概念更加高层、抽象（比如class &gt; student &gt; score [4,35,8]），小维度更加底层。给定小维度的配置（score的偏置 [8]），默认对所有高纬度概念通用（自动的将[8] 扩张 为 [1,1,8] 再扩张为 [4,35,8]，对原来的数据都按照这样一个偏置进行叠加）</p>
</li>
<li><p>When it has dim of size 1</p>
<ul>
<li>Treat it shared by all</li>
<li>[classes, students, scores] + [students, 1]</li>
</ul>
<p>理解：对于Data: [4,35,8]，对于[35,1] （对于班级上残疾学生，编号为0 ?……）希望对他们有一些偏置（每一门课都加5分）,35表现为 [5,5,0,0,……] (对 所有班级里 前两个学生 的  所有成绩  加5分，表示为[35,1]，即给出35个学生具体的偏置科目，这个偏置对所有科目适用)，把[35,1] 扩张为 [1,35,1] ，再复制为 [4,35,8]，即偏置对所有科目适用，这个偏置对所有班级适用，具体偏置取决于 35 的数据</p>
</li>
</ul>
<h4 id="Why-broadcasting"><a href="#Why-broadcasting" class="headerlink" title="Why broadcasting?"></a>Why broadcasting?</h4><p>[b,10] + [10] 这个过程，可以显示的通过 expend_dim, tf.tile 操作完成数据的扩张，如果遵循大小维度的概念，如果大维度是通用的约定的话，broadcasting 会非常简洁</p>
<ul>
<li><p>for real demanding</p>
<p>默认大维度是更加高层的概念，如果没有写高纬度的配置，则默认高纬度所有维度都适用</p>
<ul>
<li><p>[classes, students, scores]</p>
</li>
<li><p>Add bias for every student:  + 5 score</p>
<ul>
<li><p>[4,32,8]  + [4,32,8]</p>
</li>
<li><p>[4,32,8] +<font color="red"> [5.0] </font></p>
<p>[5.0] 的 shape 为 [1] 会扩张为 [4,32,8] 默认每一个班级的每个学生都适合于这个配置</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>memory consumption</p>
<p>运行时的优化手段，节省内存/显存空间</p>
</li>
<li><p>efficient and intuitive</p>
<ul>
<li>[4,32,32,3]</li>
<li>+ [3]</li>
<li>+ [32,32,1]</li>
<li>+ [4,1,1,1]</li>
</ul>
</li>
</ul>
<h4 id="Broadcastable"><a href="#Broadcastable" class="headerlink" title="Broadcastable?"></a>Broadcastable?</h4><p>是否都能 broadcast ?</p>
<ul>
<li>Match from <font color="red">Last</font> dim!<ul>
<li>if current dim=1, expand to same</li>
<li>if either has no dim, insert one dim and expand to same</li>
<li>otherwise, NOT broadcastable</li>
</ul>
</li>
</ul>
<p>右边对齐，如果相应位置上没有维度，则插入 shape 为1的维度，如果有维度，比较维度是否相等，不等则不行！</p>
<ul>
<li>Examples: ( base [4,32,14,14] )<ul>
<li>[1, 32, 1, 1]<ul>
<li>维度相等，dim=1 的维度可以扩张</li>
</ul>
</li>
<li>[14, 14]<ul>
<li>维度不等，低纬度一致，可以增加维度并扩张</li>
</ul>
</li>
<li>[<font color="red">2</font>, 32, 14, 14]  （×）<ul>
<li>维度相等，但最高维 都不是 1，不能复制！</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>理解：大小维度的概念，小维度给定数字，所有的大维度都是通用的，所有的 dim=1 的维度 默认配置对 所有个例适用</p>
</blockquote>
<h4 id="Broadcasting-1"><a href="#Broadcasting-1" class="headerlink" title="Broadcasting"></a>Broadcasting</h4><p>tf.broadcast_to 可以搞成相同的 shape，实际上这是一种最优化手段，只要操作符支持 broadcast, 就会自动判定shape不一致时，能否 broadcast 为相同的shape</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># tf.broadcast_to 无须调用 即可完成运算</span></span><br><span class="line">(x + tf.random.normal([<span class="number">3</span>])).shape <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br><span class="line">(x + tf.random.normal([<span class="number">32</span>,<span class="number">32</span>,<span class="number">1</span>])).shape <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br><span class="line">(x + tf.random.normal([<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])).shape <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br><span class="line"><span class="comment"># 也可以显式调用</span></span><br><span class="line">b = tf.broadcast_to(tf.random.normal([<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), [<span class="number">4</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line">b.shape <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不能完成的</span></span><br><span class="line">(x + tf.random.normal([<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>])).shape</span><br><span class="line"><span class="comment"># tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [4,32,32,3] vs. [1,4,1,1] [Op:AddV2] name: add/</span></span><br><span class="line"></span><br><span class="line">c = tf.broadcast_to(tf.random.normal([<span class="number">4</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]), [<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment"># tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [4,1,1,1] vs. [3,32,32,3] [Op:BroadcastTo]</span></span><br></pre></td></tr></table></figure>
<h4 id="Broadcast-VS-Tile"><a href="#Broadcast-VS-Tile" class="headerlink" title="Broadcast VS Tile"></a>Broadcast VS Tile</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">a1 = tf.broadcast_to(a, [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=4, shape=(2, 3, 4), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]],</span></span><br><span class="line"><span class="string">       [[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">a2 = tf.expand_dims(a, axis=<span class="number">0</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=6, shape=(1, 3, 4), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">a2 = tf.tile(a2, [<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=8, shape=(2, 3, 4), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]],</span></span><br><span class="line"><span class="string">       [[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># a1和a2 功能完全等价, a2 占用内存空间更大</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>索引与切片、维度变换、Broadcast 是 TensorFlow 三大核心操作。掌握后看一些复杂的神经网络结构、复杂的算法变换时，将会对这些操作背后具体的含义理解起来更加轻松</p>
</blockquote>
<h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h3><blockquote>
<p>description :</p>
<ul>
<li>+-*/</li>
<li>**, pow, square</li>
<li>sqrt</li>
<li>//, %</li>
<li>exp, log</li>
<li>@, matmul</li>
<li>linear, layer</li>
</ul>
</blockquote>
<h4 id="Operation-type"><a href="#Operation-type" class="headerlink" title="Operation type"></a>Operation type</h4><ul>
<li><p>element-wise</p>
<p>+-*/</p>
<p>对应元素的加减乘除</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">b = tf.fill([<span class="number">2</span>,<span class="number">2</span>],<span class="number">2.</span>)</span><br><span class="line">a = tf.ones([<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">a+b</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=6, shape=(2, 2), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[3., 3.],</span></span><br><span class="line"><span class="string">       [3., 3.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">a-b <span class="comment"># -1</span></span><br><span class="line">a/b <span class="comment"># 0.5</span></span><br><span class="line">b//a <span class="comment"># 2.</span></span><br><span class="line">b%a <span class="comment"># 0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数，无 tf.log API</span></span><br><span class="line">tf.math.log(a) <span class="comment"># 0.</span></span><br><span class="line"><span class="comment"># 指数</span></span><br><span class="line">tf.math.exp(a) <span class="comment"># 2.7182817</span></span><br><span class="line">tf.exp(a) <span class="comment"># 2.7182817</span></span><br><span class="line"><span class="comment"># log 2, log10 用换底公式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># n次方</span></span><br><span class="line">tf.math.pow(b,<span class="number">3</span>) <span class="comment"># 8.</span></span><br><span class="line">tf.pow(b,<span class="number">3</span>) <span class="comment"># 8.</span></span><br><span class="line">b**<span class="number">3</span> <span class="comment"># 8.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开方</span></span><br><span class="line">tf.sqrt(b) <span class="comment"># 1.4142135</span></span><br><span class="line">tf.math.sqrt(b)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>matrix-wise</p>
<ul>
<li>@</li>
<li>matmul</li>
</ul>
<p>矩阵运算多个并行计算过程：[b,3,4] @ [b,4,5]  =&gt; [b,3,5]</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">a = tf.fill([<span class="number">2</span>,<span class="number">2</span>],<span class="number">2</span>)</span><br><span class="line">b = tf.fill([<span class="number">2</span>,<span class="number">2</span>],<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 矩阵相乘1</span></span><br><span class="line">a@b</span><br><span class="line">tf.matmul(a,b)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=6, shape=(2, 2), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[12, 12],</span></span><br><span class="line"><span class="string">       [12, 12]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.fill([<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>],<span class="number">2.</span>)</span><br><span class="line"><span class="comment"># 矩阵相乘2</span></span><br><span class="line"><span class="comment"># [b,c,d] @ [d,f] = [b,c,f]</span></span><br><span class="line">a@b</span><br><span class="line">tf.matmul(a,b) <span class="comment"># TensorShape([4, 2, 5])  6.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵相乘3 —— with broadcastint</span></span><br><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.fill([<span class="number">3</span>,<span class="number">5</span>],<span class="number">2.</span>)</span><br><span class="line">bb = tf.broadcast_to(b, [<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">a@bb <span class="comment"># TensorShape([4, 2, 5])  6.</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>Recap</p>
<ul>
<li><p>$y=w*x+b$</p>
</li>
<li><p>$Y=X@W+b$</p>
</li>
<li><script type="math/tex; mode=display">
\left[
\begin{matrix}
\color{red}{x_0^0} & \color{red}{x_0^1}\\
x_1^0 & x_1^1
\end{matrix}
\right]

\left[
\begin{matrix}
w_{00} & w_{01} & w_{02} \\
w_{10} & w_{11} & w_{12}
\end{matrix}
\right]
+
\left[
\begin{matrix}
b_0,b_1,b_2
\end{matrix}
\right]
\rightarrow
\left[
\begin{matrix}
\color{red}{y_0^0} & \color{red}{y_0^1} & \color{red}{y_0^2}\\
y_1^0 & y_1^1 & y_1^2
\end{matrix}
\right]</script></li>
<li><p>$[b,2]\rightarrow[b,3]$</p>
</li>
</ul>
<p>写出矩阵形式好处：写出什么样的矩阵形式，完全取决于之前的运算逻辑。即：标量b 的形式决定了 矩阵的形式，因为矩阵的运算已经规定好，因此元素的位置需要精心的设计使得当前元素的位置在矩阵相乘的操作下满足标量形式的运算规则。也就是说，矩阵只是一个格式，希望通过这样的格式达到目的，目的即为标量形式。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.ones([<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">W = tf.ones([<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">b = tf.constant(<span class="number">0.1</span>)</span><br><span class="line">out = x@W+b</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=33, shape=(4, 1), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[2.1],</span></span><br><span class="line"><span class="string">       [2.1],</span></span><br><span class="line"><span class="string">       [2.1],</span></span><br><span class="line"><span class="string">       [2.1]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 加入非线性因子</span></span><br><span class="line">out = tf.nn.relu(out)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=36, shape=(4, 1), dtype=float32, numpy=</span></span><br><span class="line"><span class="string">array([[2.1],</span></span><br><span class="line"><span class="string">       [2.1],</span></span><br><span class="line"><span class="string">       [2.1],</span></span><br><span class="line"><span class="string">       [2.1]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>dim-wise</p>
<ul>
<li>reduce_mean</li>
<li>max</li>
<li>min</li>
<li>sum</li>
</ul>
<p>对某个维度进行操作</p>
</li>
</ul>
<h3 id="前向传播（张量）-实战"><a href="#前向传播（张量）-实战" class="headerlink" title="前向传播（张量）-实战"></a>前向传播（张量）-实战</h3><p>基本技巧：创建 Tensor，读取Tensor部分数据，Tensor维度变换，对Tensor进行数学运算，后续还会介绍高阶技巧。</p>
<h4 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h4><ul>
<li>串联 多个简单非线性层 $relu[X@W_1+b_1]$ 增加复杂度，得到输出</li>
<li>$pred=argmax(out)$</li>
<li>$loss=MSE(out, label)$</li>
<li>minimize $loss$</li>
</ul>
<h4 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h4><p>有最底层的 方式——张量直接作矩阵运算实现前向传播，后续讲解如何使用 层方式。从0开始，每个细节都掌控到，对dp理解才深刻。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x: [60k, 28, 28],</span></span><br><span class="line"><span class="comment"># y: [60k]</span></span><br><span class="line">(x, y), _ = datasets.mnist.load_data()</span><br><span class="line"><span class="comment"># x: [0~255] =&gt; [0~1.]</span></span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32) / <span class="number">255.</span></span><br><span class="line">y = tf.convert_to_tensor(y, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">print(x.shape, y.shape, x.dtype, y.dtype)</span><br><span class="line">print(tf.reduce_min(x), tf.reduce_max(x))</span><br><span class="line">print(tf.reduce_min(y), tf.reduce_max(y))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(<span class="number">128</span>)</span><br><span class="line">train_iter = iter(train_db)</span><br><span class="line">sample = next(train_iter)</span><br><span class="line"><span class="comment"># 一批 128个数据</span></span><br><span class="line">print(<span class="string">'batch:'</span>, sample[<span class="number">0</span>].shape, sample[<span class="number">1</span>].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中间的数值随便定</span></span><br><span class="line"><span class="comment"># [b, 784] =&gt; [b, 256] =&gt; [b, 128] =&gt; [b, 10]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># w shape 需要满足矩阵运算规则 [dim_in, dim_out]</span></span><br><span class="line"><span class="comment"># b shape 满足： [dim_out]</span></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>, <span class="number">256</span>], stddev=<span class="number">0.1</span>)) <span class="comment"># 方差变小，预防梯度爆炸</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))</span><br><span class="line">w2 = tf.Variable(tf.random.truncated_normal([<span class="number">256</span>, <span class="number">128</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">128</span>]))</span><br><span class="line">w3 = tf.Variable(tf.random.truncated_normal([<span class="number">128</span>, <span class="number">10</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># learn rate</span></span><br><span class="line">lr = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>): <span class="comment"># iterate db for 10</span></span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(train_db): <span class="comment"># for every batch</span></span><br><span class="line">        <span class="comment"># x:[128, 28, 28]</span></span><br><span class="line">        <span class="comment"># y: [128]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 28, 28] =&gt; [b, 28*28]</span></span><br><span class="line">        x = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自动求导过程，只会跟踪tf.Variable</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape: <span class="comment"># tf.Variable</span></span><br><span class="line">            <span class="comment"># x: [b, 28*28]</span></span><br><span class="line">            <span class="comment"># h1 = x@w1 + b1</span></span><br><span class="line">            <span class="comment"># [b, 784]@[784, 256] + [256] =&gt; [b, 256] + [256] =&gt; [b, 256] + [b, 256]</span></span><br><span class="line">            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[<span class="number">0</span>], <span class="number">256</span>])</span><br><span class="line">            h1 = tf.nn.relu(h1)</span><br><span class="line">            <span class="comment"># [b, 256] =&gt; [b, 128]</span></span><br><span class="line">            h2 = h1@w2 + b2</span><br><span class="line">            h2 = tf.nn.relu(h2)</span><br><span class="line">            <span class="comment"># [b, 128] =&gt; [b, 10]</span></span><br><span class="line">            out = h2@w3 + b3</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute loss</span></span><br><span class="line">            <span class="comment"># out: [b, 10]</span></span><br><span class="line">            <span class="comment"># y: [b] =&gt; [b, 10]</span></span><br><span class="line">            y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># mse = mean(sum(y-out)^2)</span></span><br><span class="line">            <span class="comment"># [b, 10]</span></span><br><span class="line">            loss = tf.square(y_onehot - out)</span><br><span class="line">            <span class="comment"># mean: scalar</span></span><br><span class="line">            loss = tf.reduce_mean(loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute gradients</span></span><br><span class="line">        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])</span><br><span class="line">        <span class="comment"># print(grads)</span></span><br><span class="line">        <span class="comment"># w1 = w1 - lr * w1_grad[0]</span></span><br><span class="line">        <span class="comment"># 这种情况下w1更新后 是一个新的对象，失去 tf.Variable 属性</span></span><br><span class="line">        <span class="comment"># 需要原地更新，即引用不变！</span></span><br><span class="line">        w1.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line">        b1.assign_sub(lr * grads[<span class="number">1</span>])</span><br><span class="line">        w2.assign_sub(lr * grads[<span class="number">2</span>])</span><br><span class="line">        b2.assign_sub(lr * grads[<span class="number">3</span>])</span><br><span class="line">        w3.assign_sub(lr * grads[<span class="number">4</span>])</span><br><span class="line">        b3.assign_sub(lr * grads[<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(epoch, step, <span class="string">'loss:'</span>, float(loss))</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">loss:nan 解决：</span></span><br><span class="line"><span class="string">这是 梯度爆炸 情况，这里不讲如何解决问题，牵扯内容过多，这里只给出解决方案：</span></span><br><span class="line"><span class="string">初始化的时候给一个比较好的范围</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">迭代：</span></span><br><span class="line"><span class="string">第一层：迭代数据集 10次，迭代第二次的时候已经很好，继续迭代loss在不断下降——意味着训练能达到很好的收敛状态</span></span><br><span class="line"><span class="string">    甚至迭代数百次、数千次都可以。只不过数据集总量不变，循环次数增加罢了，最终状态（天花板）是一定的</span></span><br><span class="line"><span class="string">第二层：迭代batch</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>完成了比较复杂的 三层前向传播及优化过程。</p>
<h2 id="TensorFlow-2-高阶操作"><a href="#TensorFlow-2-高阶操作" class="headerlink" title="TensorFlow 2 高阶操作"></a>TensorFlow 2 高阶操作</h2><h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><h4 id="tf-concat-拼接"><a href="#tf-concat-拼接" class="headerlink" title="tf.concat | 拼接"></a>tf.concat | 拼接</h4><ul>
<li>需要指定在哪个维度上进行合并</li>
<li>不会增加维度，因为是在原有维度上进行累加</li>
<li>所要拼接的维度可以不等，其他维度必须相等！</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># axis = 0</span></span><br><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.ones([<span class="number">2</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">c = tf.concat([a,b],axis=<span class="number">0</span>) <span class="comment"># TensorShape([6, 35, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># axis = 1</span></span><br><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">32</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.ones([<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>])</span><br><span class="line">c = tf.concat([a,b],axis=<span class="number">1</span>) <span class="comment"># TensorShape([4, 35, 8])</span></span><br></pre></td></tr></table></figure>
<h4 id="tf-stack-堆叠"><a href="#tf-stack-堆叠" class="headerlink" title="tf.stack | 堆叠"></a>tf.stack | 堆叠</h4><ul>
<li>会创建新的维度</li>
<li>要求 Tensor 所有维度相等！</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a和b来自两个 school</span></span><br><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line"><span class="comment"># concat 不会增加维度</span></span><br><span class="line">tf.concat([a,b], axis=<span class="number">-1</span>).shape <span class="comment"># TensorShape([4, 35, 16])</span></span><br><span class="line">tf.concat([a,b], axis=<span class="number">1</span>).shape <span class="comment"># TensorShape([4, 70, 8])</span></span><br><span class="line"><span class="comment"># stack创造新的维度</span></span><br><span class="line">d1 = tf.stack([a,b], axis=<span class="number">0</span>) <span class="comment"># TensorShape([2, 4, 35, 8])</span></span><br><span class="line">d2 = tf.stack([a,b],axis=<span class="number">3</span>) <span class="comment"># TensorShape([4, 35, 8, 2])</span></span><br><span class="line"><span class="comment"># axis=3时，在"8"这个概念下，再划分一个维度，分a和b两个school，也可以理解，但没有=0(大维度放前)容易理解</span></span><br></pre></td></tr></table></figure>
<p>一些维度不一致的错误</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.ones([<span class="number">3</span>,<span class="number">33</span>,<span class="number">8</span>])</span><br><span class="line">tf.concat([a,b],axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [4,35,8] vs. shape[1] = [3,33,8] [Op:ConcatV2] name: concat</span></span><br><span class="line"></span><br><span class="line">b = tf.ones([<span class="number">2</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">c = tf.concat([a,b], axis=<span class="number">0</span>) <span class="comment"># TensorShape([6, 35, 8])</span></span><br><span class="line">tf.stack([a,b], axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [4,35,8] != values[1].shape = [2,35,8] [Op:Pack] name: stack</span></span><br></pre></td></tr></table></figure>
<h3 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h3><ul>
<li>tf.split<ul>
<li>打散方式更灵活</li>
</ul>
</li>
<li>tf.unstack<ul>
<li>特性：指定axis全部打散为1，该维度消失</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">b = tf.ones([<span class="number">4</span>,<span class="number">35</span>,<span class="number">8</span>])</span><br><span class="line">c = tf.stack([a,b]) <span class="comment"># TensorShape([2, 4, 35, 8])</span></span><br><span class="line">aa, bb = tf.unstack(c, axis=<span class="number">0</span>) <span class="comment"># list</span></span><br><span class="line">aa.shape, bb.shape <span class="comment"># TensorShape([4, 35, 8])</span></span><br><span class="line"></span><br><span class="line">c.shape <span class="comment"># TensorShape([2, 4, 35, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将"8"这个维度打散</span></span><br><span class="line">res = tf.unstack(c, axis=<span class="number">3</span>) <span class="comment"># len: 8</span></span><br><span class="line">res[<span class="number">0</span>].shape <span class="comment"># TensorShape([2, 4, 35])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将"8"这个维度分割成2份</span></span><br><span class="line">res = tf.split(c, num_or_size_splits=<span class="number">2</span>, axis=<span class="number">3</span>) <span class="comment"># len: 2</span></span><br><span class="line">res[<span class="number">0</span>].shape <span class="comment"># TensorShape([2, 4, 35, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将"8"这个维度按照指定的长度打散</span></span><br><span class="line">res = tf.split(c, num_or_size_splits=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">-1</span>], axis=<span class="number">3</span>)</span><br><span class="line">res[<span class="number">0</span>].shape <span class="comment"># TensorShape([2, 4, 35, 2])</span></span><br><span class="line">res[<span class="number">1</span>].shape <span class="comment"># TensorShape([2, 4, 35, 3])</span></span><br><span class="line">res[<span class="number">2</span>].shape <span class="comment"># TensorShape([2, 4, 35, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h3><h4 id="tf-norm"><a href="#tf-norm" class="headerlink" title="tf.norm"></a>tf.norm</h4><p>向量范数</p>
<script type="math/tex; mode=display">
\bold{Eukl.\ Norm}\quad\left\|x\right\|_2=[\sum_k x_k^2]^{\frac{1}{2}} \\
\bold{Max.norm}\quad \left\|x\right\|_\infty=\max_k|x_k| \\
\bold{L_1-Norm}\quad \left\|x\right\|_1=\sum_k|x_k|</script><h5 id="二范数-tf-norm-x"><a href="#二范数-tf-norm-x" class="headerlink" title="二范数 tf.norm(x)"></a>二范数 tf.norm(x)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]], dtype=tf.float32)</span><br><span class="line"><span class="comment"># sqrt(45)</span></span><br><span class="line">tf.norm(a) <span class="comment"># &lt;tf.Tensor: id=12, shape=(), dtype=float32, numpy=6.708204&gt;</span></span><br><span class="line">tf.sqrt(tf.reduce_sum(tf.square(a))) <span class="comment"># 同上</span></span><br><span class="line"></span><br><span class="line">b = tf.ones([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line">tf.norm(b) <span class="comment"># 96.99484</span></span><br><span class="line">tf.sqrt(tf.reduce_sum(tf.square(b))) <span class="comment"># 96.99484</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定axis</span></span><br><span class="line">tf.norm(a, ord=<span class="number">2</span>, axis=<span class="number">1</span>) <span class="comment">#[sqrt(30.),sqrt(15.)]</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=34, shape=(2,), dtype=float32, numpy=array([5.477226 , 3.8729835], dtype=float32)&gt;</span></span><br></pre></td></tr></table></figure>
<h5 id="L1-Norm"><a href="#L1-Norm" class="headerlink" title="L1 Norm"></a>L1 Norm</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ord=1</span></span><br><span class="line">a = tf.constant([[<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]], dtype=tf.float32) <span class="comment"># TensorShape([2, 4])</span></span><br><span class="line">tf.norm(a, ord=<span class="number">1</span>) <span class="comment"># 17.0</span></span><br><span class="line">tf.norm(a, ord=<span class="number">1</span>, axis=<span class="number">0</span>) <span class="comment"># [5., 2., 5., 5.]</span></span><br><span class="line">tf.norm(a, ord=<span class="number">1</span>, axis=<span class="number">1</span>) <span class="comment"># [10.,  7.]</span></span><br></pre></td></tr></table></figure>
<p>矩阵范数不讨论……</p>
<h4 id="tf-reduce-min-max-mean"><a href="#tf-reduce-min-max-mean" class="headerlink" title="tf.reduce_min/max/mean"></a>tf.reduce_min/max/mean</h4><p>最值、平均值，reduce是提醒 该过程会降维</p>
<ul>
<li>axis 指定后，会在该参数轴上求最值，同时该轴的维度消失…</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>]], dtype=tf.float32) <span class="comment"># TensorShape([2, 4])</span></span><br><span class="line"><span class="comment"># 不指定axis 结果为 scatter</span></span><br><span class="line">tf.reduce_max(a) <span class="comment"># 4.0</span></span><br><span class="line">tf.reduce_min(a) <span class="comment"># 1.0</span></span><br><span class="line">tf.reduce_mean(a) <span class="comment"># 2.125</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># axis=0 ==&gt; TensorShape([4])</span></span><br><span class="line">tf.reduce_max(a, axis=<span class="number">0</span>) <span class="comment"># [4., 1., 3., 3.]</span></span><br><span class="line">tf.reduce_min(a, axis=<span class="number">0</span>) <span class="comment"># [1., 1., 2., 2.]</span></span><br><span class="line">tf.reduce_mean(a, axis=<span class="number">0</span>) <span class="comment"># [2.5, 1. , 2.5, 2.5]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># axis=1 ==&gt; TensorShape([2])</span></span><br><span class="line">tf.reduce_max(a, axis=<span class="number">1</span>) <span class="comment"># [4., 3.]</span></span><br><span class="line">tf.reduce_min(a, axis=<span class="number">1</span>) <span class="comment"># [1., 1.]</span></span><br><span class="line">tf.reduce_mean(a, axis=<span class="number">1</span>) <span class="comment"># [2.5 , 1.75]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># axis=2 ==&gt; InvalidArgumentError: Invalid reduction dimension (2 for input with 2 dimension(s) [Op:Mean]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],dtype=tf.int32,maxval=<span class="number">10</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[[6, 4, 3, 6, 7],</span></span><br><span class="line"><span class="string">        [8, 7, 0, 2, 2],</span></span><br><span class="line"><span class="string">        [0, 7, 3, 7, 7],</span></span><br><span class="line"><span class="string">        [4, 5, 4, 9, 6]],</span></span><br><span class="line"><span class="string">       [[7, 4, 5, 1, 7],</span></span><br><span class="line"><span class="string">        [4, 8, 7, 6, 1],</span></span><br><span class="line"><span class="string">        [3, 6, 5, 0, 5],</span></span><br><span class="line"><span class="string">        [1, 0, 8, 7, 7]],</span></span><br><span class="line"><span class="string">       [[1, 2, 3, 9, 4],</span></span><br><span class="line"><span class="string">        [0, 8, 6, 4, 0],</span></span><br><span class="line"><span class="string">        [4, 6, 2, 9, 2],</span></span><br><span class="line"><span class="string">        [4, 5, 7, 2, 0]]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.reduce_max(a) <span class="comment"># 9</span></span><br><span class="line">tf.reduce_max(a, axis=<span class="number">0</span>) <span class="comment"># TensorShape([4, 5])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[7, 4, 5, 9, 7],</span></span><br><span class="line"><span class="string"> [8, 8, 7, 6, 2],</span></span><br><span class="line"><span class="string"> [4, 7, 5, 9, 7],</span></span><br><span class="line"><span class="string"> [4, 5, 8, 9, 7]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.reduce_max(a, axis=<span class="number">1</span>) <span class="comment"># TensorShape([3, 5])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[8, 7, 4, 9, 7],</span></span><br><span class="line"><span class="string"> [7, 8, 8, 7, 7],</span></span><br><span class="line"><span class="string"> [4, 8, 7, 9, 4]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.reduce_max(a, axis=<span class="number">2</span>) <span class="comment"># TensorShape([3, 4])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[7, 8, 7, 9],</span></span><br><span class="line"><span class="string"> [7, 8, 6, 8],</span></span><br><span class="line"><span class="string"> [9, 8, 9, 7]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>理解：将shape写出来后，axis 所在的那个维度，进行操作后维度消失。</p>
<h4 id="tf-argmax-argmin"><a href="#tf-argmax-argmin" class="headerlink" title="tf.argmax/argmin"></a>tf.argmax/argmin</h4><p>最值位置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a.shape <span class="comment"># TensorShape([3, 4, 5])</span></span><br><span class="line">tf.argmax(a) <span class="comment"># TensorShape([4, 5])</span></span><br><span class="line">tf.argmax(a,axis=<span class="number">0</span>) <span class="comment"># 默认axis=0</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[1, 0, 1, 2, 0],</span></span><br><span class="line"><span class="string"> [0, 1, 1, 1, 0],</span></span><br><span class="line"><span class="string"> [2, 0, 1, 2, 0],</span></span><br><span class="line"><span class="string"> [0, 0, 1, 0, 1]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.argmax(a,axis=<span class="number">1</span>) <span class="comment"># TensorShape([3, 5])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[1, 1, 3, 3, 0],</span></span><br><span class="line"><span class="string"> [0, 1, 3, 3, 0],</span></span><br><span class="line"><span class="string"> [2, 1, 3, 0, 0]],</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.argmax(a,axis=<span class="number">2</span>) <span class="comment"># TensorShape([3, 4])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[4, 0, 1, 3],</span></span><br><span class="line"><span class="string"> [0, 1, 1, 2],</span></span><br><span class="line"><span class="string"> [3, 1, 3, 2]]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="tf-equal"><a href="#tf-equal" class="headerlink" title="tf.equal"></a>tf.equal</h4><p>比较</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">b = tf.range(<span class="number">5</span>)</span><br><span class="line">res = tf.equal(a,b)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=5, shape=(5,), dtype=bool, numpy=array([False, False, False, False, False])&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相同结果数目，可以得到 准确度</span></span><br><span class="line">tf.reduce_sum(tf.cast(res, dtype=tf.int32)) <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line"><span class="comment"># shape: [2,3] 表示2个样本预测 每个输出 的概率</span></span><br><span class="line">a = tf.constant([[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.7</span>],</span><br><span class="line">                 [<span class="number">0.9</span>,<span class="number">0.05</span>,<span class="number">0.05</span>]])</span><br><span class="line"><span class="comment"># 根据概率求预测值，即将概率最大的作为预测值</span></span><br><span class="line">pred = tf.cast(tf.argmax(a, axis=<span class="number">1</span>), dtype=tf.int32)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=14, shape=(2,), dtype=int32, numpy=array([2, 0])&gt;</span></span><br><span class="line"></span><br><span class="line">y = tf.constant([<span class="number">2</span>,<span class="number">1</span>], dtype=tf.int32)</span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=17, shape=(2,), dtype=int32, numpy=array([2, 1])&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测正确的个数</span></span><br><span class="line">correct = tf.reduce_sum(tf.cast(tf.equal(y,pred), dtype=tf.int32)) <span class="comment"># 1</span></span><br><span class="line">accuracy = correct / <span class="number">2</span></span><br><span class="line"><span class="comment"># &lt;tf.Tensor: id=25, shape=(), dtype=float64, numpy=0.5&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="tf-unique"><a href="#tf-unique" class="headerlink" title="tf.unique"></a>tf.unique</h4><p>去除重复值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">4</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>])</span><br><span class="line">tf.unique(a)</span><br><span class="line"><span class="comment"># Unique(y=&lt;tf.Tensor: id=1, shape=(3,), dtype=int32, numpy=array([4, 2, 3])&gt;, idx=&lt;tf.Tensor: id=2, shape=(5,), dtype=int32, numpy=array([0, 1, 1, 0, 2])&gt;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 还原</span></span><br><span class="line">aa = tf.gather(d.y, d.idx)</span><br></pre></td></tr></table></figure>
<h3 id="张量排序"><a href="#张量排序" class="headerlink" title="张量排序"></a>张量排序</h3><h4 id="Sort-argsort"><a href="#Sort-argsort" class="headerlink" title="Sort/argsort"></a>Sort/argsort</h4><p>默认 axis = -1</p>
<ul>
<li>sort: 完全排序：对一个序列完整的排序</li>
<li>argsort: 返回 大小顺序的 idx</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.shuffle(tf.range(<span class="number">5</span>)) <span class="comment"># [3, 1, 2, 0, 4]</span></span><br><span class="line">tf.sort(a, direction=<span class="string">"DESCENDING"</span>) <span class="comment"># [4, 3, 2, 1, 0]</span></span><br><span class="line">tf.sort(a) <span class="comment"># [0, 1, 2, 3, 4] 默认ASCENDING</span></span><br><span class="line"></span><br><span class="line">idx = tf.argsort(a, direction=<span class="string">"DESCENDING"</span>) <span class="comment"># [4, 0, 2, 1, 3]</span></span><br><span class="line">tf.argsort(a) <span class="comment"># [3, 1, 2, 0, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得idx后有 gather 还原</span></span><br><span class="line">tf.gather(a, idx) <span class="comment"># [4, 3, 2, 1, 0]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.uniform([<span class="number">3</span>,<span class="number">3</span>], maxval=<span class="number">10</span>, dtype=tf.int32)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[1, 5, 2],</span></span><br><span class="line"><span class="string">       [3, 2, 9],</span></span><br><span class="line"><span class="string">       [2, 8, 2]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.sort(a)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[1, 2, 5],</span></span><br><span class="line"><span class="string">       [2, 3, 9],</span></span><br><span class="line"><span class="string">       [2, 2, 8]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">idx = tf.argsort(a)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 2, 1],</span></span><br><span class="line"><span class="string">       [1, 0, 2],</span></span><br><span class="line"><span class="string">       [0, 2, 1]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="TopK"><a href="#TopK" class="headerlink" title="TopK"></a>TopK</h4><p>如果对 Tensor全部进行排序，可能会耗费很长时间。有时候仅需要取前几个值</p>
<p>API: <span class="exturl" data-url="aHR0cHM6Ly90ZW5zb3JmbG93Lmdvb2dsZS5jbi9hcGlfZG9jcy9weXRob24vdGYvbWF0aC90b3Bfaw==" title="https://tensorflow.google.cn/api_docs/python/tf/math/top_k">https://tensorflow.google.cn/api_docs/python/tf/math/top_k<i class="fa fa-external-link"></i></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">res = tf.math.top_k(a,<span class="number">2</span>) <span class="comment"># type:tensorflow.python.ops.gen_nn_ops.TopKV2</span></span><br><span class="line">res.indices</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=94, shape=(3, 2), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[1, 2],</span></span><br><span class="line"><span class="string">       [2, 0],</span></span><br><span class="line"><span class="string">       [1, 0]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">res.values</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">&lt;tf.Tensor: id=93, shape=(3, 2), dtype=int32, numpy=</span></span><br><span class="line"><span class="string">array([[5, 2],</span></span><br><span class="line"><span class="string">       [9, 3],</span></span><br><span class="line"><span class="string">       [8, 2]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="Top-k-Acc"><a href="#Top-k-Acc" class="headerlink" title="Top-k Acc."></a>Top-k Acc.</h4><ul>
<li>Prob: [0.1, 0.2, <font color="red">0.3, 0.4</font>]</li>
<li>Label: [2]</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">consider top k</th>
<th style="text-align:center">prediction</th>
<th style="text-align:center">accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">[3]</td>
<td style="text-align:center">0/1=0%</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">[3,2]</td>
<td style="text-align:center">100%</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">[3,2,1]</td>
<td style="text-align:center">100%</td>
</tr>
</tbody>
</table>
</div>
<p>保证 前k个 中有一个是对的就说明准确。Top-1 最严格，但表现不是很好，例如 ImageNet 中，Top-1 可能就70%，可以考虑 Top-5，可以作为算法性能指标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实战：计算Top-k Acc.</span></span><br><span class="line"><span class="comment"># 并假设索引值即为预测值</span></span><br><span class="line">prob = tf.constant([[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.7</span>], <span class="comment"># 第一个样本预测结果 2&gt;1&gt;0</span></span><br><span class="line">                    [<span class="number">0.2</span>,<span class="number">0.7</span>,<span class="number">0.1</span>]])<span class="comment"># 第二个样本预测结果 1&gt;0&gt;2</span></span><br><span class="line"></span><br><span class="line">target = tf.constant([<span class="number">2</span>,<span class="number">0</span>]) <span class="comment"># 假设实际的结果分别为 2,0</span></span><br><span class="line"></span><br><span class="line">k_b = tf.math.top_k(prob, <span class="number">3</span>).indices</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[2, 1, 0],</span></span><br><span class="line"><span class="string">       [1, 0, 2]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 转置</span></span><br><span class="line">k_b = tf.transpose(k_b, [<span class="number">1</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 第一行是Top1, 第二行是Top2 ……</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[2, 1],</span></span><br><span class="line"><span class="string">       [1, 0],</span></span><br><span class="line"><span class="string">       [0, 2]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">target = tf.broadcast_to(target, [<span class="number">3</span>,<span class="number">2</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[2, 0],</span></span><br><span class="line"><span class="string">       [2, 0],</span></span><br><span class="line"><span class="string">       [2, 0]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span>  os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line">tf.random.set_seed(<span class="number">2467</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span><span class="params">(output, target, topk=<span class="params">(<span class="number">1</span>,)</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算 Top-K准确度</span></span><br><span class="line"><span class="string">    :param output: 预测值</span></span><br><span class="line"><span class="string">    :param target: 真实值</span></span><br><span class="line"><span class="string">    :param topk: 输出的top-k类别，e.g. (1,2)则会输出[top1,top2]</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    maxk = max(topk)</span><br><span class="line">    <span class="comment"># 样本总数</span></span><br><span class="line">    batch_size = target.shape[<span class="number">0</span>]</span><br><span class="line">	<span class="comment"># 为了跟 target 对应，需要进行转置操作 </span></span><br><span class="line">    pred = tf.math.top_k(output, maxk).indices</span><br><span class="line">    pred = tf.transpose(pred, perm=[<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 为了与pred进行比较来 计算top-k, 需要扩张成 pred 的shape</span></span><br><span class="line">    target_ = tf.broadcast_to(target, pred.shape)</span><br><span class="line">    <span class="comment"># [10, b]</span></span><br><span class="line">    correct = tf.equal(pred, target_)</span><br><span class="line"></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> topk:</span><br><span class="line">        <span class="comment"># Top-n 中仅有一个是对的！</span></span><br><span class="line">        correct_k = tf.cast(tf.reshape(correct[:k], [<span class="number">-1</span>]), dtype=tf.float32)</span><br><span class="line">        <span class="comment"># 累加求和即可得到正确的个数</span></span><br><span class="line">        correct_k = tf.reduce_sum(correct_k)</span><br><span class="line">        acc = float(correct_k* (<span class="number">100.0</span> / batch_size) )</span><br><span class="line">        res.append(acc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10个样本，6类</span></span><br><span class="line">output = tf.random.normal([<span class="number">10</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># 使得6类总和为1</span></span><br><span class="line">output = tf.math.softmax(output, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 0-5随机 label作为 target</span></span><br><span class="line">target = tf.random.uniform([<span class="number">10</span>], maxval=<span class="number">6</span>, dtype=tf.int32)</span><br><span class="line">print(<span class="string">'prob:'</span>, output.numpy())</span><br><span class="line">pred = tf.argmax(output, axis=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">'pred:'</span>, pred.numpy())</span><br><span class="line">print(<span class="string">'label:'</span>, target.numpy())</span><br><span class="line"></span><br><span class="line">acc = accuracy(output, target, topk=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">print(<span class="string">'top-1-6 acc:'</span>, acc)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">prob: [[<span class="number">0.25310278</span> <span class="number">0.21715644</span> <span class="number">0.16043882</span> <span class="number">0.13088997</span> <span class="number">0.04334083</span> <span class="number">0.19507109</span>]</span><br><span class="line"> [<span class="number">0.05892418</span> <span class="number">0.04548917</span> <span class="number">0.00926314</span> <span class="number">0.14529602</span> <span class="number">0.66777605</span> <span class="number">0.07325139</span>]</span><br><span class="line"> [<span class="number">0.09742808</span> <span class="number">0.08304427</span> <span class="number">0.07460099</span> <span class="number">0.04067177</span> <span class="number">0.626185</span>   <span class="number">0.07806987</span>]</span><br><span class="line"> [<span class="number">0.20478569</span> <span class="number">0.12294924</span> <span class="number">0.12010485</span> <span class="number">0.13751231</span> <span class="number">0.36418733</span> <span class="number">0.05046057</span>]</span><br><span class="line"> [<span class="number">0.11872064</span> <span class="number">0.31072393</span> <span class="number">0.12530336</span> <span class="number">0.1552888</span>  <span class="number">0.2132587</span>  <span class="number">0.07670452</span>]</span><br><span class="line"> [<span class="number">0.01519807</span> <span class="number">0.09672114</span> <span class="number">0.1460476</span>  <span class="number">0.00934331</span> <span class="number">0.5649092</span>  <span class="number">0.16778067</span>]</span><br><span class="line"> [<span class="number">0.04199061</span> <span class="number">0.18141054</span> <span class="number">0.06647632</span> <span class="number">0.6006175</span>  <span class="number">0.03198383</span> <span class="number">0.07752118</span>]</span><br><span class="line"> [<span class="number">0.09226219</span> <span class="number">0.2346089</span>  <span class="number">0.13022321</span> <span class="number">0.16295874</span> <span class="number">0.05362028</span> <span class="number">0.3263266</span> ]</span><br><span class="line"> [<span class="number">0.07019574</span> <span class="number">0.0861177</span>  <span class="number">0.10912605</span> <span class="number">0.10521299</span> <span class="number">0.2152082</span>  <span class="number">0.4141393</span> ]</span><br><span class="line"> [<span class="number">0.01882887</span> <span class="number">0.26597694</span> <span class="number">0.19122466</span> <span class="number">0.24109262</span> <span class="number">0.14920162</span> <span class="number">0.13367532</span>]]</span><br><span class="line">pred: [<span class="number">0</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">1</span> <span class="number">4</span> <span class="number">3</span> <span class="number">5</span> <span class="number">5</span> <span class="number">1</span>]</span><br><span class="line">label: [<span class="number">0</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">2</span> <span class="number">4</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">5</span>]</span><br><span class="line">top<span class="number">-1</span><span class="number">-6</span> acc: [<span class="number">40.0</span>, <span class="number">40.0</span>, <span class="number">50.0</span>, <span class="number">70.0</span>, <span class="number">80.0</span>, <span class="number">100.0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="填充和复制"><a href="#填充和复制" class="headerlink" title="填充和复制"></a>填充和复制</h3><h4 id="pad"><a href="#pad" class="headerlink" title="pad"></a>pad</h4><p>当shape为 [3, 3]，ndim=2 时，paddings参数为填充方式</p>
<p>paddings: [top，bottom], [left, right]</p>
<p>猜测：总共的方括号有 ndim 个，每个ndim表示在该维度上进行填充，里边的元素个数 为 2；就拿本例来说：ndim=2，axis=0 是行的概念，第一个[]就在行的上下进行填充，第二个[] 就在列的左右进行填充</p>
<p>具体事例看API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">a = tf.reshape(tf.range(<span class="number">9</span>),[<span class="number">3</span>,<span class="number">-1</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.pad(a, [[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 1, 2, 0],</span></span><br><span class="line"><span class="string">       [3, 4, 5, 0],</span></span><br><span class="line"><span class="string">       [6, 7, 8, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.pad(a, [[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 0, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 1, 2, 0],</span></span><br><span class="line"><span class="string">       [0, 3, 4, 5, 0],</span></span><br><span class="line"><span class="string">       [0, 6, 7, 8, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0, 0]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<p>Image padding</p>
<ul>
<li>一般情况下，卷积(converlutional)操作得到的size（called B）会略小于原来的size(called A)，把 A padding 为 A’，使得A’ 卷积后的size (called B) 与A 相等</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设4张照片</span></span><br><span class="line">a = tf.random.normal([<span class="number">4</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">3</span>])</span><br><span class="line">b = tf.pad(a,</span><br><span class="line">           [[<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">            [<span class="number">2</span>,<span class="number">2</span>], <span class="comment"># 每张照片上下填充2行</span></span><br><span class="line">            [<span class="number">2</span>,<span class="number">2</span>], <span class="comment"># 每张照片左右填充2列</span></span><br><span class="line">            [<span class="number">0</span>,<span class="number">0</span>]]</span><br><span class="line">          )</span><br><span class="line">b.shape <span class="comment"># TensorShape([4, 32, 32, 3])</span></span><br></pre></td></tr></table></figure>
<p>NLP padding</p>
<ul>
<li>为了使每个句子单词数量等长，需要padding，成为定长的句子。</li>
</ul>
<h4 id="tile"><a href="#tile" class="headerlink" title="tile"></a>tile</h4><p>真实的数据复制操作</p>
<ul>
<li>repeat data along dim <font color="red">n</font> times</li>
<li>[a,b,c], 2</li>
<li>-&gt; [a,b,c,a,b,c]</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">a = tf.reshape(tf.range(<span class="number">9</span>),[<span class="number">3</span>,<span class="number">-1</span>]) <span class="comment"># TensorShape([3, 3])</span></span><br><span class="line">tf.tile(a, [<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.tile(a, [<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8],</span></span><br><span class="line"><span class="string">       [0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># Inner dim first</span></span><br><span class="line">tf.tile(a, [<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8, 6, 7, 8],</span></span><br><span class="line"><span class="string">       [0, 1, 2, 0, 1, 2],</span></span><br><span class="line"><span class="string">       [3, 4, 5, 3, 4, 5],</span></span><br><span class="line"><span class="string">       [6, 7, 8, 6, 7, 8]])</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="broadcast-to"><a href="#broadcast-to" class="headerlink" title="broadcast_to"></a>broadcast_to</h4><p>隐式复制，并没有真正复制，略</p>
<p>另外再次说明：有些操作符支持 broadcast，不需要显式调用也可以。比如 [4,3] + [3]</p>
<h3 id="张量限幅"><a href="#张量限幅" class="headerlink" title="张量限幅"></a>张量限幅</h3><h4 id="clip-by-value"><a href="#clip-by-value" class="headerlink" title="clip_by_value"></a>clip_by_value</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.range(<span class="number">10</span>) 		<span class="comment"># [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line"><span class="comment"># 单边限幅</span></span><br><span class="line">tf.maximum(a,<span class="number">2</span>)  		<span class="comment"># [2, 2, 2, 3, 4, 5, 6, 7, 8, 9]  max(a,2)</span></span><br><span class="line">tf.minimum(a,<span class="number">8</span>) 		<span class="comment"># [0, 1, 2, 3, 4, 5, 6, 7, 8, 8]  min(a,8)</span></span><br><span class="line"><span class="comment"># 双边限幅</span></span><br><span class="line">tf.clip_by_value(a,<span class="number">2</span>,<span class="number">8</span>)	<span class="comment"># [2, 2, 2, 3, 4, 5, 6, 7, 8, 8]  min(8, max(a,2))</span></span><br></pre></td></tr></table></figure>
<h4 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = a - <span class="number">5</span> <span class="comment"># [-5, -4, -3, -2, -1,  0,  1,  2,  3,  4]</span></span><br><span class="line">tf.nn.relu(a) <span class="comment"># [0, 0, 0, 0, 0, 0, 1, 2, 3, 4]</span></span><br><span class="line">tf.maximum(a,<span class="number">0</span>) <span class="comment"># 同上</span></span><br></pre></td></tr></table></figure>
<h4 id="clip-by-norm"><a href="#clip-by-norm" class="headerlink" title="clip_by_norm"></a>clip_by_norm</h4><blockquote>
<p> descriptions: Clips tensor values to a maximum L2-norm. </p>
</blockquote>
<p>对于 vector（假设为(x,y) ） 有方向的概念，gradient 代表函数值增长最快的方向，在 gradient clipping 时（根据值来裁剪），如果裁剪x，不裁剪y，相当于改变了 vector 的方向，而同时希望 不改变 gradient 的方向。即：</p>
<ul>
<li>将值限制到较小的范围内</li>
<li>不改变 gradient 的方向</li>
</ul>
<p>我们可以采用等比例放缩：</p>
<script type="math/tex; mode=display">
\frac{(x,y)}{\|(x,y)\|}*clip\_norm</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = tf.random.normal([<span class="number">2</span>,<span class="number">2</span>], mean=<span class="number">10</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[11.795398, 11.040575],</span></span><br><span class="line"><span class="string"> [ 9.601366, 11.682478]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.norm(a) <span class="comment"># 22.12899</span></span><br><span class="line">aa = tf.clip_by_norm(a, <span class="number">15</span>)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[7.995438 , 7.483786 ],</span></span><br><span class="line"><span class="string"> [6.508227 , 7.9188957]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">tf.norm(aa) <span class="comment"># 15.0</span></span><br></pre></td></tr></table></figure>
<h4 id="gradient-clipping"><a href="#gradient-clipping" class="headerlink" title="gradient clipping"></a>gradient clipping</h4><p>Deep Learning 就是一个 gradient 的学科，所有的设计，不管是  批量归一化(Batch Normalization) 还是 学习率(Learning rate) 或者其他优化，选择 Adam 等 优化器，都是要创造良好的环境使得 gradient 能够很好的引导网络走向优良的方向（loss 下降的方向），这其中有两大障碍：</p>
<ul>
<li>gradient Exploding 值太大，步长长，导致来回震荡</li>
<li>gradient vanishing 值太小，很长时间原地不动</li>
</ul>
<p>对比 MINIST 数据集，很难看到这些障碍，这是因为数据太简单了，稍微复杂的情况，这两种情况很容易出现。为了复现这个问题，可以 set $lr=1$ </p>
<blockquote>
<p>体验使用 gradient clipping 使神经网络 运行的更好</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保证整体的 vector 方向不变，整理缩放比例一致</span></span><br><span class="line">new_grads, total_norm = tf.clip_by_global_norm(grad, <span class="comment"># t_list</span></span><br><span class="line">                                               <span class="number">25</span>, <span class="comment"># clip_norm</span></span><br><span class="line">                                               use_norm=<span class="literal">None</span>,</span><br><span class="line">                                               name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 复现 gradient 问题 完整代码略</span></span><br><span class="line">(x, y), _ = tf.keras.datasets.mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使 MNIST 出现 gradient 问题</span></span><br><span class="line">x = tf.convert_to_tensor(x, dtype=tf.float32) / <span class="number">50.</span></span><br><span class="line">optimizer = optimizers.SGD(lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute gradient</span></span><br><span class="line">grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])</span><br><span class="line"></span><br><span class="line"><span class="comment"># before clip</span></span><br><span class="line">print(<span class="string">'==before=='</span>)</span><br><span class="line"><span class="keyword">for</span> g <span class="keyword">in</span> grads:</span><br><span class="line">    print(tf.norm(g))</span><br><span class="line"></span><br><span class="line"><span class="comment"># clip_by_global_norm</span></span><br><span class="line">grads, _ = tf.clip_by_global_norm(grads, <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># after clip</span></span><br><span class="line">print(<span class="string">'==after=='</span>)</span><br><span class="line"><span class="keyword">for</span> g <span class="keyword">in</span> grads:</span><br><span class="line">    print(tf.norm(g))</span><br></pre></td></tr></table></figure>
<p>使 MNIST 出现 gradient 问题：</p>
<ul>
<li>learning rate 设置大一些 0.01</li>
<li>输入限制到 0-5</li>
</ul>
<p>如果没有加入 clip_by_global_norm ，可能出现如下输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.0</span><span class="number">.0</span></span><br><span class="line">x: (<span class="number">60000</span>, <span class="number">28</span>, <span class="number">28</span>) y: (<span class="number">60000</span>, <span class="number">10</span>)</span><br><span class="line">sample: (<span class="number">128</span>, <span class="number">28</span>, <span class="number">28</span>) (<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">85.98511</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.4999373</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">119.83714</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.1436949</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">132.3766</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.3793144</span>, shape=(), dtype=float32)</span><br><span class="line"><span class="number">0</span> loss: <span class="number">27.218223571777344</span></span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">1364.3158</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">42.74355</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1548.8491</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">28.97616</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1484.8851</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">20.308739</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">193.77467</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">6.0404367</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">684.8617</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">6.861237</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">278.7067</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">6.103272</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">1023.0613</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">33.209797</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2905.0632</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">30.36965</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1529.8531</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">19.110842</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">460.32898</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">14.685878</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1999.5978</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">14.543871</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1581.0513</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">14.16413</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">242151.9</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">7056.0005</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">208297.53</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">1655.4941</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">275000.88</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">371.19373</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">5674674700.0</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">168005580.0</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">17948512000.0</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">8654475.0</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">7781239000.0</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">200822.3</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">5.2273057e+17</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(inf, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(nan, shape=(), dtype=float32)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="number">100</span> loss: nan</span><br><span class="line"><span class="number">200</span> loss: nan</span><br><span class="line"><span class="number">300</span> loss: nan</span><br><span class="line"><span class="number">400</span> loss: nan</span><br><span class="line"><span class="number">500</span> loss: nan</span><br></pre></td></tr></table></figure>
<p>这一现象是 gradient Exploding ，加入 clip 后这一情况会得到改善</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">82.41478</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.5339637</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">112.82711</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.1518593</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">133.68774</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">2.4968529</span>, shape=(), dtype=float32)</span><br><span class="line">==after==</span><br><span class="line">tf.Tensor(<span class="number">6.391338</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.19651109</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">8.749841</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.16687858</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">10.367601</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.19363312</span>, shape=(), dtype=float32)</span><br><span class="line"><span class="number">0</span> loss: <span class="number">26.43914031982422</span></span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">29.091652</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.75837916</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">33.95014</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.5890228</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">36.167934</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.65747374</span>, shape=(), dtype=float32)</span><br><span class="line">==after==</span><br><span class="line">tf.Tensor(<span class="number">7.586649</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.19777344</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">8.853666</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.15360793</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">9.432032</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.17145889</span>, shape=(), dtype=float32)</span><br><span class="line">==before==</span><br><span class="line">tf.Tensor(<span class="number">32.28737</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.9371307</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">42.480125</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.8141749</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">49.03783</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.90850127</span>, shape=(), dtype=float32)</span><br><span class="line">==after==</span><br><span class="line">tf.Tensor(<span class="number">6.6815042</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.19392857</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">8.79078</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.16848427</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">10.147824</span>, shape=(), dtype=float32)</span><br><span class="line">tf.Tensor(<span class="number">0.18800406</span>, shape=(), dtype=float32)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="number">0</span> loss: <span class="number">28.069189071655273</span></span><br><span class="line"><span class="number">100</span> loss: <span class="number">0.45802319049835205</span></span><br><span class="line"><span class="number">200</span> loss: <span class="number">0.1636555790901184</span></span><br><span class="line"><span class="number">300</span> loss: <span class="number">0.10513024032115936</span></span><br><span class="line"><span class="number">400</span> loss: <span class="number">0.09373171627521515</span></span><br><span class="line"><span class="number">500</span> loss: <span class="number">0.08014819771051407</span></span><br><span class="line"><span class="number">600</span> loss: <span class="number">0.08581136912107468</span></span><br><span class="line"><span class="number">700</span> loss: <span class="number">0.08369828015565872</span></span><br><span class="line"><span class="number">800</span> loss: <span class="number">0.07317458838224411</span></span><br><span class="line"><span class="number">900</span> loss: <span class="number">0.06344582885503769</span></span><br><span class="line"><span class="number">1000</span> loss: <span class="number">0.06460664421319962</span></span><br><span class="line"><span class="number">1100</span> loss: <span class="number">0.06235339492559433</span></span><br><span class="line"><span class="number">1200</span> loss: <span class="number">0.05849233269691467</span></span><br><span class="line"><span class="number">1300</span> loss: <span class="number">0.06183977425098419</span></span><br><span class="line"><span class="number">1400</span> loss: <span class="number">0.05234844610095024</span></span><br><span class="line"><span class="number">1500</span> loss: <span class="number">0.05308971926569939</span></span><br><span class="line"><span class="number">1600</span> loss: <span class="number">0.061960265040397644</span></span><br><span class="line"><span class="number">1700</span> loss: <span class="number">0.06077909469604492</span></span><br><span class="line"><span class="number">1800</span> loss: <span class="number">0.0626436173915863</span></span><br><span class="line"><span class="number">1900</span> loss: <span class="number">0.05248890072107315</span></span><br><span class="line"><span class="number">2000</span> loss: <span class="number">0.05477441847324371</span></span><br><span class="line"><span class="number">2100</span> loss: <span class="number">0.050197310745716095</span></span><br><span class="line"><span class="number">2200</span> loss: <span class="number">0.05866188555955887</span></span><br><span class="line"><span class="number">2300</span> loss: <span class="number">0.047177381813526154</span></span><br><span class="line"><span class="number">2400</span> loss: <span class="number">0.04807071015238762</span></span><br><span class="line"><span class="number">2500</span> loss: <span class="number">0.050419218838214874</span></span><br><span class="line"><span class="number">2600</span> loss: <span class="number">0.05031692981719971</span></span><br></pre></td></tr></table></figure>
<p>在自己做研究的时候，两种梯度问题会非常严重，这时候就需要采用 clip_by_global_norm 解决</p>
<h3 id="高阶OP"><a href="#高阶OP" class="headerlink" title="高阶OP"></a>高阶OP</h3><h4 id="where"><a href="#where" class="headerlink" title="where"></a>where</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">    condition,</span><br><span class="line">    x=<span class="literal">None</span>,</span><br><span class="line">    y=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>If both <code>x</code> and <code>y</code> are None, then this operation returns the coordinates of true elements of <code>condition</code>.  The coordinates are returned in a 2-D tensor where the first dimension (rows) represents the number of true elements, and the second dimension (columns) represents the coordinates of the true elements. Keep in mind, the shape of the output tensor can vary depending on how many true values there are in input. Indices are output in row-major order. </p>
<p>If both non-None, <code>condition</code>, <code>x</code> and <code>y</code> must be broadcastable to the same shape.</p>
<p>The <code>condition</code> tensor acts as a mask that chooses, based on the value at each element, whether the corresponding element / row in the output should be taken from <code>x</code> (if true) or <code>y</code> (if false).</p>
</blockquote>
<ul>
<li><p>如果一个参数，即只给定 condition，则返回 True 所在的坐标（coordinates）</p>
</li>
<li><p>如果三个参数，根据 condition 中的true/false进行选择值，true 则从x中取，false则从y中取</p>
<p>assert：这里的 true 是相对的，非0即true</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.where第一种用法：取坐标</span></span><br><span class="line"></span><br><span class="line">a = tf.random.normal([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[ 0.524617  ,  1.2183112 , -0.22141142],</span></span><br><span class="line"><span class="string">       [-1.361392  , -0.98102844,  1.4004889 ],</span></span><br><span class="line"><span class="string">       [-1.1087296 ,  0.35734025, -0.6342832 ]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">mask = a &gt; <span class="number">0</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[ True,  True, False],</span></span><br><span class="line"><span class="string">       [False, False,  True],</span></span><br><span class="line"><span class="string">       [False,  True, False]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 以下两种实现是为了有目的性的选择</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method1：根据mask 直接通过boolean_mask取值</span></span><br><span class="line"><span class="comment"># 取得 mask 上为 True 的 在 a 上对应位置的元素</span></span><br><span class="line">tf.boolean_mask(a, mask) <span class="comment"># [0.524617  , 1.2183112 , 1.4004889 , 0.35734025]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># method2：通过mask得到坐标，再用gather_nd 取值</span></span><br><span class="line"><span class="comment"># 查询 True 的坐标</span></span><br><span class="line">indices = tf.where(mask)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[0, 0],</span></span><br><span class="line"><span class="string">       [0, 1],</span></span><br><span class="line"><span class="string">       [1, 2],</span></span><br><span class="line"><span class="string">       [2, 1]], dtype=int64)</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 结合 gather_nd 取值，结果跟 boolean_mask 一致</span></span><br><span class="line">tf.gather_nd(a, indices) <span class="comment"># [0.524617  , 1.2183112 , 1.4004889 , 0.35734025]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf.where 第二种用法：取值</span></span><br><span class="line">mask</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[ True,  True, False],</span></span><br><span class="line"><span class="string">       [False, False,  True],</span></span><br><span class="line"><span class="string">       [False,  True, False]])</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">A = tf.ones([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">B = tf.zeros([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">tf.where(mask, A, B)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[1., 1., 0.],</span></span><br><span class="line"><span class="string">       [0., 0., 1.],</span></span><br><span class="line"><span class="string">       [0., 1., 0.]], dtype=float32)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="scatter-nd"><a href="#scatter-nd" class="headerlink" title="scatter_nd"></a>scatter_nd</h4><p>根据坐标有目的性的更新，底板是全0，更新内容为：在 updates 上按照 indices 上的顺序和坐标进行更新。</p>
<p>使用灵活，但不是很easy</p>
<p><span class="exturl" data-url="aHR0cHM6Ly90ZW5zb3JmbG93Lmdvb2dsZS5jbi9hcGlfZG9jcy9weXRob24vdGYvc2NhdHRlcl9uZA==" title="https://tensorflow.google.cn/api_docs/python/tf/scatter_nd">https://tensorflow.google.cn/api_docs/python/tf/scatter_nd<i class="fa fa-external-link"></i></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">indices = tf.constant([[<span class="number">4</span>], [<span class="number">3</span>], [<span class="number">1</span>], [<span class="number">7</span>]])</span><br><span class="line">updates = tf.constant([<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line">shape = tf.constant([<span class="number">8</span>])</span><br><span class="line">scatter = tf.scatter_nd(indices, updates, shape)</span><br><span class="line"><span class="comment"># [0, 11, 0, 10, 9, 0, 0, 12]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A + scatter 即指定位置上相加</span></span><br></pre></td></tr></table></figure>
<p>如果更新A的一部分值，先A要更新的一部分值取出来，更新到地板上后得到A1，A-A1 就会把要更新的内容清零，再把新的值写到底板A2，然后 清零后的+A2 即可。</p>
<p>TensorFlow 并不是专门为用户设计，而是内部实现能达到最精简的原则。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">indices = tf.constant([[<span class="number">0</span>], [<span class="number">2</span>]])</span><br><span class="line">updates = tf.constant([[[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                        [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">                        [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]],</span><br><span class="line">                       [[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">                        [<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">                        [<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>],</span><br><span class="line">                        [<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]]])</span><br><span class="line">shape = tf.constant([<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line">scatter = tf.scatter_nd(indices, updates, shape)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[[5, 5, 5, 5],</span></span><br><span class="line"><span class="string">        [6, 6, 6, 6],</span></span><br><span class="line"><span class="string">        [7, 7, 7, 7],</span></span><br><span class="line"><span class="string">        [8, 8, 8, 8]],</span></span><br><span class="line"><span class="string">       [[0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0]],</span></span><br><span class="line"><span class="string">       [[5, 5, 5, 5],</span></span><br><span class="line"><span class="string">        [6, 6, 6, 6],</span></span><br><span class="line"><span class="string">        [7, 7, 7, 7],</span></span><br><span class="line"><span class="string">        [8, 8, 8, 8]],</span></span><br><span class="line"><span class="string">       [[0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0],</span></span><br><span class="line"><span class="string">        [0, 0, 0, 0]]])&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
<h4 id="meshgrid"><a href="#meshgrid" class="headerlink" title="meshgrid"></a>meshgrid</h4><p>给定 x 和 y 的范围，生成点的范围</p>
<p>Points: </p>
<ul>
<li>x: -2 ~ 2，5个点</li>
<li>y: -2 ~ 2，5个点</li>
<li>Tensor:  [25,2] 25个点，每个点的坐标有2个数字表示</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numpy</span></span><br><span class="line"><span class="comment"># 没有GPU加速，无法与tf深度结合在一起</span></span><br><span class="line">points = []</span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> np.linspace(<span class="number">-2</span>,<span class="number">2</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> np.linspace(<span class="number">-2</span>,<span class="number">2</span>,<span class="number">5</span>):</span><br><span class="line">        points.append([x,y])</span><br><span class="line"><span class="keyword">return</span> np.array(points) <span class="comment"># (25, 2)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPU acceleration</span></span><br><span class="line">y = tf.linspace(<span class="number">-2.</span>,<span class="number">2</span>,<span class="number">5</span>) <span class="comment"># [-2., -1.,  0.,  1.,  2.]</span></span><br><span class="line">x = tf.linspace(<span class="number">-2.</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">points_x, points_y = tf.meshgrid(x, y) <span class="comment"># TensorShape([5, 5])</span></span><br><span class="line"></span><br><span class="line">points_x</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[-2., -1.,  0.,  1.,  2.],</span></span><br><span class="line"><span class="string">       [-2., -1.,  0.,  1.,  2.],</span></span><br><span class="line"><span class="string">       [-2., -1.,  0.,  1.,  2.],</span></span><br><span class="line"><span class="string">       [-2., -1.,  0.,  1.,  2.],</span></span><br><span class="line"><span class="string">       [-2., -1.,  0.,  1.,  2.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">points_y</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">array([[-2., -2., -2., -2., -2.],</span></span><br><span class="line"><span class="string">       [-1., -1., -1., -1., -1.],</span></span><br><span class="line"><span class="string">       [ 0.,  0.,  0.,  0.,  0.],</span></span><br><span class="line"><span class="string">       [ 1.,  1.,  1.,  1.,  1.],</span></span><br><span class="line"><span class="string">       [ 2.,  2.,  2.,  2.,  2.]], dtype=float32)&gt;</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># 如果拼接在一起就是shape为 [5,5,2] 的Tensor</span></span><br><span class="line">points = tf.stack([points_x, points_y], axis=<span class="number">2</span>)</span><br><span class="line">points.shape <span class="comment"># TensorShape([5, 5, 2])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># meshgrid.py</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :param x: [b, 2]</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    z = tf.math.sin(x[...,<span class="number">0</span>]) + tf.math.sin(x[...,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">x = tf.linspace(<span class="number">0.</span>, <span class="number">2</span>*<span class="number">3.14</span>, <span class="number">500</span>)</span><br><span class="line">y = tf.linspace(<span class="number">0.</span>, <span class="number">2</span>*<span class="number">3.14</span>, <span class="number">500</span>)</span><br><span class="line"><span class="comment"># [50, 50]</span></span><br><span class="line">point_x, point_y = tf.meshgrid(x, y)</span><br><span class="line"><span class="comment"># [50, 50, 2]</span></span><br><span class="line">points = tf.stack([point_x, point_y], axis=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># points = tf.reshape(points, [-1, 2])</span></span><br><span class="line">print(<span class="string">'points:'</span>, points.shape)</span><br><span class="line">z = func(points)</span><br><span class="line">print(<span class="string">'z:'</span>, z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 已经得到x,y 和其对应的 z 的值，可以进行画图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">plt.figure(<span class="string">'plot 2d func value'</span>)</span><br><span class="line">plt.imshow(z, origin=<span class="string">'lower'</span>, interpolation=<span class="string">'none'</span>)</span><br><span class="line">plt.colorbar()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等高线图像</span></span><br><span class="line">plt.figure(<span class="string">'plot 2d func contour'</span>)</span><br><span class="line">plt.contour(point_x, point_y, z)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>好处：并行生成对应坐标轴系，而不需要 通过 for 循环 生成</p>

    </div>

    
    
    
        
      
        <div id="reward-container">
  <div></div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.png" alt="xiaohuihui 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.png" alt="xiaohuihui 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>xiaohuihui</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://xiaohuihui1024.github.io/2019/10/21/深度学习与TensorFlow 2入门实战（一）基础部分/" title="深度学习与TensorFlow 2入门实战（一）基础部分">https://xiaohuihui1024.github.io/2019/10/21/深度学习与TensorFlow 2入门实战（一）基础部分/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
            
              <a href="/tags/TensorFlow-2-0/" rel="tag"># TensorFlow 2.0</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/06/12/MongoDB学习笔记/" rel="next" title="MongoDB学习笔记">
                  <i class="fa fa-chevron-left"></i> MongoDB学习笔记
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习初见"><span class="nav-number">1.</span> <span class="nav-text">深度学习初见</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-简介"><span class="nav-number">1.1.</span> <span class="nav-text">TensorFlow 简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-vs-PyTorch"><span class="nav-number">1.2.</span> <span class="nav-text">TensorFlow vs PyTorch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-2-0"><span class="nav-number">1.3.</span> <span class="nav-text">TensorFlow 2.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#忘掉1-0"><span class="nav-number">1.4.</span> <span class="nav-text">忘掉1.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow-2-0-eco-system"><span class="nav-number">1.5.</span> <span class="nav-text">TensorFlow 2.0 eco-system</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习建议"><span class="nav-number">1.6.</span> <span class="nav-text">学习建议</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-TensorFlow"><span class="nav-number">1.7.</span> <span class="nav-text">why TensorFlow</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开发环境安装"><span class="nav-number">2.</span> <span class="nav-text">开发环境安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Step1-Anaconda"><span class="nav-number">2.1.</span> <span class="nav-text">Step1 Anaconda</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step2-CUDA-10-0"><span class="nav-number">2.2.</span> <span class="nav-text">Step2 CUDA 10.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Step3-TensorFlow安装"><span class="nav-number">2.3.</span> <span class="nav-text">Step3. TensorFlow安装</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow-2-0-基本操作"><span class="nav-number">3.</span> <span class="nav-text">TensorFlow 2.0 基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据载体比较"><span class="nav-number">3.1.</span> <span class="nav-text">数据载体比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#What-‘s-Tensor"><span class="nav-number">3.2.</span> <span class="nav-text">What ‘s Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Flow-in-Graph"><span class="nav-number">3.3.</span> <span class="nav-text">Tensor Flow in Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-is-computing-lib"><span class="nav-number">3.4.</span> <span class="nav-text">TF is computing lib</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建Tensor"><span class="nav-number">3.5.</span> <span class="nav-text">创建Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#From-NumPy，List"><span class="nav-number">3.5.1.</span> <span class="nav-text">From NumPy，List</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zeros-ones"><span class="nav-number">3.5.2.</span> <span class="nav-text">zeros, ones</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#zeros-like"><span class="nav-number">3.5.3.</span> <span class="nav-text">zeros_like</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fill"><span class="nav-number">3.5.4.</span> <span class="nav-text">Fill</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tf-random-下的分布"><span class="nav-number">3.5.4.1.</span> <span class="nav-text">tf.random 下的分布</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normal"><span class="nav-number">3.5.5.</span> <span class="nav-text">Normal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Uniform"><span class="nav-number">3.5.6.</span> <span class="nav-text">Uniform</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Random-Permutation"><span class="nav-number">3.5.7.</span> <span class="nav-text">Random Permutation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#constant"><span class="nav-number">3.5.8.</span> <span class="nav-text">constant</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Typecal-Dim-Data"><span class="nav-number">3.5.9.</span> <span class="nav-text">Typecal Dim Data</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Scalar-标量"><span class="nav-number">3.5.9.1.</span> <span class="nav-text">Scalar |  标量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Vector-向量"><span class="nav-number">3.5.9.2.</span> <span class="nav-text">Vector | 向量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Matrix-矩阵"><span class="nav-number">3.5.9.3.</span> <span class="nav-text">Matrix | 矩阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dim-3-Tensor"><span class="nav-number">3.5.9.4.</span> <span class="nav-text">Dim=3 Tensor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dim-4-Tensor"><span class="nav-number">3.5.9.5.</span> <span class="nav-text">Dim=4 Tensor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dim-5-Tensor"><span class="nav-number">3.5.9.6.</span> <span class="nav-text">Dim=5 Tensor</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Property"><span class="nav-number">3.6.</span> <span class="nav-text">Tensor Property</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Check-Tensor-Type"><span class="nav-number">3.7.</span> <span class="nav-text">Check Tensor Type</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#判断一个变量是不是-Tensor"><span class="nav-number">3.7.1.</span> <span class="nav-text">判断一个变量是不是 Tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#判断-Tensor-类型"><span class="nav-number">3.7.2.</span> <span class="nav-text">判断 Tensor 类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#类型转化"><span class="nav-number">3.7.3.</span> <span class="nav-text">类型转化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Variable"><span class="nav-number">3.8.</span> <span class="nav-text">tf.Variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#To-numpy"><span class="nav-number">3.9.</span> <span class="nav-text">To numpy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#索引"><span class="nav-number">3.10.</span> <span class="nav-text">索引</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-indexing"><span class="nav-number">3.10.1.</span> <span class="nav-text">Basic indexing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NumPy-style-indexing"><span class="nav-number">3.10.2.</span> <span class="nav-text">NumPy-style indexing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#切片"><span class="nav-number">3.11.</span> <span class="nav-text">切片</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#start-end"><span class="nav-number">3.11.1.</span> <span class="nav-text">start:end</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#start-end-step"><span class="nav-number">3.11.2.</span> <span class="nav-text">start: end: step</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1"><span class="nav-number">3.11.3.</span> <span class="nav-text">::-1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#…"><span class="nav-number">3.11.4.</span> <span class="nav-text">…</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Selective-Indexing"><span class="nav-number">3.12.</span> <span class="nav-text">Selective Indexing</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-gather"><span class="nav-number">3.12.1.</span> <span class="nav-text">tf.gather</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-gather-nd"><span class="nav-number">3.12.2.</span> <span class="nav-text">tf.gather_nd</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-boolean-mask"><span class="nav-number">3.12.3.</span> <span class="nav-text">tf.boolean_mask</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#维度变换"><span class="nav-number">3.13.</span> <span class="nav-text">维度变换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#View-amp-Reshape"><span class="nav-number">3.13.1.</span> <span class="nav-text">View &amp; Reshape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#content-amp-transpose"><span class="nav-number">3.13.2.</span> <span class="nav-text">content &amp; transpose</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Squeeze-VS-Expand-dims"><span class="nav-number">3.13.3.</span> <span class="nav-text">Squeeze VS Expand_dims</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Expand-dim"><span class="nav-number">3.13.3.1.</span> <span class="nav-text">Expand dim</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Squeeze-dim"><span class="nav-number">3.13.3.2.</span> <span class="nav-text">Squeeze dim</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Broadcasting"><span class="nav-number">3.14.</span> <span class="nav-text">Broadcasting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-understand"><span class="nav-number">3.14.1.</span> <span class="nav-text">How to understand?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Why-broadcasting"><span class="nav-number">3.14.2.</span> <span class="nav-text">Why broadcasting?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Broadcastable"><span class="nav-number">3.14.3.</span> <span class="nav-text">Broadcastable?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Broadcasting-1"><span class="nav-number">3.14.4.</span> <span class="nav-text">Broadcasting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Broadcast-VS-Tile"><span class="nav-number">3.14.5.</span> <span class="nav-text">Broadcast VS Tile</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数学运算"><span class="nav-number">3.15.</span> <span class="nav-text">数学运算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Operation-type"><span class="nav-number">3.15.1.</span> <span class="nav-text">Operation type</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播（张量）-实战"><span class="nav-number">3.16.</span> <span class="nav-text">前向传播（张量）-实战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recap"><span class="nav-number">3.16.1.</span> <span class="nav-text">Recap</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实战"><span class="nav-number">3.16.2.</span> <span class="nav-text">实战</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow-2-高阶操作"><span class="nav-number">4.</span> <span class="nav-text">TensorFlow 2 高阶操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Merge"><span class="nav-number">4.1.</span> <span class="nav-text">Merge</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-concat-拼接"><span class="nav-number">4.1.1.</span> <span class="nav-text">tf.concat | 拼接</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-stack-堆叠"><span class="nav-number">4.1.2.</span> <span class="nav-text">tf.stack | 堆叠</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Split"><span class="nav-number">4.2.</span> <span class="nav-text">Split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据统计"><span class="nav-number">4.3.</span> <span class="nav-text">数据统计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-norm"><span class="nav-number">4.3.1.</span> <span class="nav-text">tf.norm</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#二范数-tf-norm-x"><span class="nav-number">4.3.1.1.</span> <span class="nav-text">二范数 tf.norm(x)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#L1-Norm"><span class="nav-number">4.3.1.2.</span> <span class="nav-text">L1 Norm</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-reduce-min-max-mean"><span class="nav-number">4.3.2.</span> <span class="nav-text">tf.reduce_min/max/mean</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-argmax-argmin"><span class="nav-number">4.3.3.</span> <span class="nav-text">tf.argmax/argmin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-equal"><span class="nav-number">4.3.4.</span> <span class="nav-text">tf.equal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-unique"><span class="nav-number">4.3.5.</span> <span class="nav-text">tf.unique</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量排序"><span class="nav-number">4.4.</span> <span class="nav-text">张量排序</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sort-argsort"><span class="nav-number">4.4.1.</span> <span class="nav-text">Sort/argsort</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TopK"><span class="nav-number">4.4.2.</span> <span class="nav-text">TopK</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Top-k-Acc"><span class="nav-number">4.4.3.</span> <span class="nav-text">Top-k Acc.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#填充和复制"><span class="nav-number">4.5.</span> <span class="nav-text">填充和复制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pad"><span class="nav-number">4.5.1.</span> <span class="nav-text">pad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tile"><span class="nav-number">4.5.2.</span> <span class="nav-text">tile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#broadcast-to"><span class="nav-number">4.5.3.</span> <span class="nav-text">broadcast_to</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#张量限幅"><span class="nav-number">4.6.</span> <span class="nav-text">张量限幅</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#clip-by-value"><span class="nav-number">4.6.1.</span> <span class="nav-text">clip_by_value</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#relu"><span class="nav-number">4.6.2.</span> <span class="nav-text">relu</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#clip-by-norm"><span class="nav-number">4.6.3.</span> <span class="nav-text">clip_by_norm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gradient-clipping"><span class="nav-number">4.6.4.</span> <span class="nav-text">gradient clipping</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高阶OP"><span class="nav-number">4.7.</span> <span class="nav-text">高阶OP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#where"><span class="nav-number">4.7.1.</span> <span class="nav-text">where</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#scatter-nd"><span class="nav-number">4.7.2.</span> <span class="nav-text">scatter_nd</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#meshgrid"><span class="nav-number">4.7.3.</span> <span class="nav-text">meshgrid</span></a></li></ol></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar.jpg"
      alt="xiaohuihui">
  <p class="site-author-name" itemprop="name">xiaohuihui</p>
  <div class="site-description" itemprop="description">记录学习历程，分享技术心得。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/rss.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3hpYW9odWlodWkxMDI0" title="GitHub &rarr; https://github.com/xiaohuihui1024"><i class="fa fa-fw fa-github"></i>GitHub</span>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <span class="exturl" data-url="bWFpbHRvOnpoYW5neXVodWljaG5AMTYzLmNvbQ==" title="E-Mail &rarr; mailto:zhangyuhuichn@163.com"><i class="fa fa-fw fa-envelope"></i>E-Mail</span>
      </span>
    
  </div>
  <div class="cc-license motion-element" itemprop="license">
    
  
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC8="><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      常用链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly9nb29nbGUuY29tLw==" title="https://google.com/">Google</span>
        </li>
      
        <li class="links-of-blogroll-item">
          <span class="exturl" data-url="aHR0cHM6Ly93d3cuYmFpZHUuY29tLw==" title="https://www.baidu.com/">Baidu</span>
        </li>
      
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiaohuihui</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">103k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:07</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0Lm9yZw==">NexT.Gemini</span> v7.4.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/js/next-boot.js?v=7.4.0"></script>



  








  <script src="/js/local-search.js?v=7.4.0"></script>














  

  

  

</body>
</html>
